<!DOCTYPE html><html lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><title>Eli | CSC 246: Operating Systems</title><link href=/styles.css rel=stylesheet><link href=/favicon-32.png rel=icon sizes=32x32><link href=/favicon-128.png rel=icon sizes=128x128><link href=/favicon-180.png rel=icon sizes=180x180><link href=/favicon-192.png rel=icon sizes=192x192><style id=MJX-CHTML-styles>mjx-container[jax=CHTML]{line-height:0}mjx-container [space="1"]{margin-left:.111em}mjx-container [space="2"]{margin-left:.167em}mjx-container [space="3"]{margin-left:.222em}mjx-container [space="4"]{margin-left:.278em}mjx-container [space="5"]{margin-left:.333em}mjx-container [rspace="1"]{margin-right:.111em}mjx-container [rspace="2"]{margin-right:.167em}mjx-container [rspace="3"]{margin-right:.222em}mjx-container [rspace="4"]{margin-right:.278em}mjx-container [rspace="5"]{margin-right:.333em}mjx-container [size="s"]{font-size:70.7%}mjx-container [size=ss]{font-size:50%}mjx-container [size=Tn]{font-size:60%}mjx-container [size=sm]{font-size:85%}mjx-container [size=lg]{font-size:120%}mjx-container [size=Lg]{font-size:144%}mjx-container [size=LG]{font-size:173%}mjx-container [size=hg]{font-size:207%}mjx-container [size=HG]{font-size:249%}mjx-container [width=full]{width:100%}mjx-box{display:inline-block}mjx-block{display:block}mjx-itable{display:inline-table}mjx-row{display:table-row}mjx-row>*{display:table-cell}mjx-mtext{display:inline-block}mjx-mstyle{display:inline-block}mjx-merror{display:inline-block;color:red;background-color:#ff0}mjx-mphantom{visibility:hidden}_::-webkit-full-page-media,_:future,:root mjx-container{will-change:opacity}mjx-assistive-mml{position:absolute!important;top:0;left:0;clip:rect(1px,1px,1px,1px);padding:1px 0 0 0!important;border:0!important;display:block!important;width:auto!important;overflow:hidden!important;-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}mjx-assistive-mml[display=block]{width:100%!important}mjx-math{display:inline-block;text-align:left;line-height:0;text-indent:0;font-style:normal;font-weight:400;font-size:100%;font-size-adjust:none;letter-spacing:normal;border-collapse:collapse;word-wrap:normal;word-spacing:normal;white-space:nowrap;direction:ltr;padding:1px 0}mjx-container[jax=CHTML][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=CHTML][display=true][width=full]{display:flex}mjx-container[jax=CHTML][display=true] mjx-math{padding:0}mjx-container[jax=CHTML][justify=left]{text-align:left}mjx-container[jax=CHTML][justify=right]{text-align:right}mjx-mi{display:inline-block;text-align:left}mjx-c{display:inline-block}mjx-utext{display:inline-block;padding:.75em 0 .2em 0}mjx-msub{display:inline-block;text-align:left}mjx-TeXAtom{display:inline-block;text-align:left}mjx-mo{display:inline-block;text-align:left}mjx-stretchy-h{display:inline-table;width:100%}mjx-stretchy-h>*{display:table-cell;width:0}mjx-stretchy-h>*>mjx-c{display:inline-block;transform:scalex(1.0000001)}mjx-stretchy-h>*>mjx-c::before{display:inline-block;width:initial}mjx-stretchy-h>mjx-ext{overflow:hidden;overflow:clip visible;width:100%}mjx-stretchy-h>mjx-ext>mjx-c::before{transform:scalex(500)}mjx-stretchy-h>mjx-ext>mjx-c{width:0}mjx-stretchy-h>mjx-beg>mjx-c{margin-right:-.1em}mjx-stretchy-h>mjx-end>mjx-c{margin-left:-.1em}mjx-stretchy-v{display:inline-block}mjx-stretchy-v>*{display:block}mjx-stretchy-v>mjx-beg{height:0}mjx-stretchy-v>mjx-end>mjx-c{display:block}mjx-stretchy-v>*>mjx-c{transform:scaley(1.0000001);transform-origin:left center;overflow:hidden}mjx-stretchy-v>mjx-ext{display:block;height:100%;box-sizing:border-box;border:0 solid transparent;overflow:hidden;overflow:visible clip}mjx-stretchy-v>mjx-ext>mjx-c::before{width:initial;box-sizing:border-box}mjx-stretchy-v>mjx-ext>mjx-c{transform:scaleY(500) translateY(.075em);overflow:visible}mjx-mark{display:inline-block;height:0}mjx-mn{display:inline-block;text-align:left}mjx-msup{display:inline-block;text-align:left}mjx-c::before{display:block;width:0}.MJX-TEX{font-family:MJXZERO,MJXTEX}.TEX-B{font-family:MJXZERO,MJXTEX-B}.TEX-I{font-family:MJXZERO,MJXTEX-I}.TEX-MI{font-family:MJXZERO,MJXTEX-MI}.TEX-BI{font-family:MJXZERO,MJXTEX-BI}.TEX-S1{font-family:MJXZERO,MJXTEX-S1}.TEX-S2{font-family:MJXZERO,MJXTEX-S2}.TEX-S3{font-family:MJXZERO,MJXTEX-S3}.TEX-S4{font-family:MJXZERO,MJXTEX-S4}.TEX-A{font-family:MJXZERO,MJXTEX-A}.TEX-C{font-family:MJXZERO,MJXTEX-C}.TEX-CB{font-family:MJXZERO,MJXTEX-CB}.TEX-FR{font-family:MJXZERO,MJXTEX-FR}.TEX-FRB{font-family:MJXZERO,MJXTEX-FRB}.TEX-SS{font-family:MJXZERO,MJXTEX-SS}.TEX-SSB{font-family:MJXZERO,MJXTEX-SSB}.TEX-SSI{font-family:MJXZERO,MJXTEX-SSI}.TEX-SC{font-family:MJXZERO,MJXTEX-SC}.TEX-T{font-family:MJXZERO,MJXTEX-T}.TEX-V{font-family:MJXZERO,MJXTEX-V}.TEX-VB{font-family:MJXZERO,MJXTEX-VB}mjx-stretchy-h mjx-c,mjx-stretchy-v mjx-c{font-family:MJXZERO,MJXTEX-S1,MJXTEX-S4,MJXTEX,MJXTEX-A!important}@font-face{font-family:MJXZERO;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff) format("woff")}@font-face{font-family:MJXTEX;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-B;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff) format("woff")}@font-face{font-family:MJXTEX-I;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff) format("woff")}@font-face{font-family:MJXTEX-MI;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff) format("woff")}@font-face{font-family:MJXTEX-BI;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff) format("woff")}@font-face{font-family:MJXTEX-S1;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-S2;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-S3;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-S4;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-A;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-C;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-CB;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff) format("woff")}@font-face{font-family:MJXTEX-FR;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-FRB;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff) format("woff")}@font-face{font-family:MJXTEX-SS;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-SSB;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff) format("woff")}@font-face{font-family:MJXTEX-SSI;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff) format("woff")}@font-face{font-family:MJXTEX-SC;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-T;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-V;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff) format("woff")}@font-face{font-family:MJXTEX-VB;src:url(https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff) format("woff")}mjx-c.mjx-c1D70F.TEX-I::before{padding:.431em .517em .013em 0;content:"\3C4"}mjx-c.mjx-c1D44F.TEX-I::before{padding:.694em .429em .011em 0;content:"b"}mjx-c.mjx-c1D6FC.TEX-I::before{padding:.442em .64em .011em 0;content:"\3B1"}mjx-c.mjx-c1D45B.TEX-I::before{padding:.442em .6em .011em 0;content:"n"}mjx-c.mjx-c2B::before{padding:.583em .778em .082em 0;content:"+"}mjx-c.mjx-c31::before{padding:.666em .5em 0 0;content:"1"}mjx-c.mjx-c3D::before{padding:.583em .778em .082em 0;content:"="}mjx-c.mjx-c28::before{padding:.75em .389em .25em 0;content:"("}mjx-c.mjx-c2212::before{padding:.583em .778em .082em 0;content:"\2212"}mjx-c.mjx-c29::before{padding:.75em .389em .25em 0;content:")"}mjx-c.mjx-c1D45E.TEX-I::before{padding:.442em .46em .194em 0;content:"q"}mjx-c.mjx-c2F::before{padding:.75em .5em .25em 0;content:"/"}mjx-c.mjx-c1D464.TEX-I::before{padding:.443em .716em .011em 0;content:"w"}mjx-c.mjx-c1D45F.TEX-I::before{padding:.442em .451em .011em 0;content:"r"}mjx-c.mjx-c30::before{padding:.666em .5em .022em 0;content:"0"}mjx-c.mjx-c2264::before{padding:.636em .778em .138em 0;content:"\2264"}mjx-c.mjx-c1D446.TEX-I::before{padding:.705em .645em .022em 0;content:"S"}mjx-c.mjx-c1D461.TEX-I::before{padding:.626em .361em .011em 0;content:"t"}mjx-c.mjx-c210E.TEX-I::before{padding:.694em .576em .011em 0;content:"h"}mjx-c.mjx-c1D456.TEX-I::before{padding:.661em .345em .011em 0;content:"i"}mjx-c.mjx-c1D45A.TEX-I::before{padding:.442em .878em .011em 0;content:"m"}mjx-c.mjx-c1D460.TEX-I::before{padding:.442em .469em .01em 0;content:"s"}mjx-c.mjx-c1D459.TEX-I::before{padding:.694em .298em .011em 0;content:"l"}mjx-c.mjx-c1D452.TEX-I::before{padding:.442em .466em .011em 0;content:"e"}mjx-c.mjx-c1D45C.TEX-I::before{padding:.441em .485em .011em 0;content:"o"}mjx-c.mjx-c1D466.TEX-I::before{padding:.442em .49em .205em 0;content:"y"}mjx-c.mjx-c32::before{padding:.666em .5em 0 0;content:"2"}mjx-c.mjx-c1D45D.TEX-I::before{padding:.442em .503em .194em 0;content:"p"}mjx-c.mjx-c2E::before{padding:.12em .278em 0 0;content:"."}</style><nav><h2><a href=/ >e l i</a></h2><ul><li><a href=/blog/ >blog</a><li><a href=/projects/ >projects</a><li><a href=/notes/ >notes</a></ul></nav><article><header><h1 class=title>CSC 246: Operating Systems</h1><p>Instructor: Dr. David Sturgil | Semester: Spring 2020<p><h3>Table of Contents</h3><ul><li><a href=#terms>Terms</a><li><a href=#computer-architecture>Computer Architecture</a><li><a href=#os-architecture>OS Architecture</a><li><a href=#inter-process-communication-ipc>Inter-Process Communication (IPC)</a><li><a href=#processes>Processes</a><li><a href=#threads>Threads</a><li><a href=#cpu-scheduling>CPU Scheduling</a><li><a href=#synchronization>Synchronization</a><li><a href=#deadlock>Deadlock</a><li><a href=#memory-1>Memory</a><li><a href=#gpu-programming-cuda>GPU Programming & CUDA</a><li><a href=#distributed-systems>Distributed Systems</a><li><a href=#protection>Protection</a></ul></header><h1 id=terms><a href=#terms aria-label="anchor link for terms">#</a> Terms</h1><ul><li>Operating System: Creates a layer of separation between hardware and application. A program that helps other programs run.<ul><li>Virtualizes hardware, normally mandatory.<li>Provides system calls (syscalls).</ul><li>Virtualization: Abstraction over underlying hardware, making it easier to use, share, and reason about. For example, UNIX's <em>everything is a file</em> abstraction.<li>System Calls: Requests the operating system to do something. This can be expensive (because of interrupts) and obtuse, so normally you call these through a library.</ul><h1 id=computer-architecture><a href=#computer-architecture aria-label="anchor link for computer-architecture">#</a> Computer Architecture</h1><p>In this class, we'll consider a simplified model of a computer because they're complex.<p>We also follow the Von Neumann architecture, which is the processor and memory model, where you put program in memory.<ul><li>CPU: Processor executing your code.<li>CPU Cache: Tiny amount of incredibly fast memory your CPU uses. Amortizes cost of memory accesses.<ul><li>Cache Hit: CPU finds what it wants in the cache.<li>Cache Miss: CPU does not find what it wants in the cache.</ul><li>Memory: Volatile memory containing your working program and data.<li>Device Controllers: Control specific hardware, varies wildly depending on computer.<ul><li>Includes persistent storage, such as disk controllers.</ul></ul><figure><img src=simple_computer_architecture.png><figcaption><p>Simple Computer Architecture</figcaption></figure><h2 id=storage-hierarchy><a href=#storage-hierarchy aria-label="anchor link for storage-hierarchy">##</a> Storage Hierarchy</h2><p>Sadly, we haven't found any storage device that is faster, cheaper, and larger than every other option. Because of this, we have to make engineering decisions to improve performance.<p>We usually do this by creating a <strong>storage hierarchy</strong> of memory going from small and fast to large and slow. We tend to use the smaller, faster memory <strong>cache</strong> to temporarily store data for the larger, slower memory <strong>backing store</strong> to try to get the best of both worlds.<h2 id=cpu-cache><a href=#cpu-cache aria-label="anchor link for cpu-cache">##</a> CPU Cache</h2><p>Programs normally exhibit <strong>temporal locality</strong>, where they are likely to access things accessed recently, and <strong>spatial locality</strong>, where they are like to access things near things accessed recently.<p>Since we normally both <em>read</em> and <em>write</em> to the cache, the backing store may become out of date. To resolve this, we need a write-policy to keep them in step.<ul><li>Write-through: The hardware automatically writes to the backing store (RAM) whenever the cache is written to.<li>Write-back: Whenever (a portion of) the cache is cleared / replaced, write it back to the backing store to get updates.</ul><h2 id=device-controllers><a href=#device-controllers aria-label="anchor link for device-controllers">##</a> Device Controllers</h2><p>Here's a quick breakdown on how programs can access hardware.<ul><li>System call: OS receives requests from application.<li>Command: OS makes requests to hardware.<li>Interrupts: OS is notified when hardware needs service.<li>Upcall: OS can notify application of events of interest.</ul><figure><img src=switching_layers.png><figcaption><p>Overview of Layers</figcaption></figure><h3 id=cpu-to-device><a href=#cpu-to-device aria-label="anchor link for cpu-to-device">###</a> CPU to Device</h3><p>Most device controllers have a few registers to communicate with the CPU.<ul><li>Status registers: To check what it’s doing.<li>Data-in registers: To give it some data.<li>Data-out registers: To get some data back.<li>Control registers: To tell it what to do next.</ul><p>That's cool and all, but how do you tell the CPU to use these? There's two main ways.<ul><li>Port-Mapped I/O: Special CPU instructions for IO. Somewhat common on embedded systems with a set number of devices.<li>Memory-Mapped I/O: Computer hardware reserves a part of address space for devices. The CPU then accesses these as if they were just normal memory, but the hardware redirects the requests.</ul><h3 id=devices-to-cpu><a href=#devices-to-cpu aria-label="anchor link for devices-to-cpu">###</a> Devices to CPU</h3><p>There are a few ways for the device to tell the CPU about things, like completing a request or something.<ul><li>Polling: The CPU must know to look at the status registers to check out the device. <em>Can be inefficient if you're polling a ton.</em><li>Interrupts: Hardware supported system. The hardware fires off an interrupt and the CPU will automatically do jump to an interrupt handler.<ul><li>Interrupt Handler: A subroutine that runs whenever an interrupt is fired. Can be arbitrarily long, but should be short to prevent latency spikes.<li>Interrupt Vector: List of addresses of subroutines to run when interrupt fires off.</ul></ul><p><em>Note:</em> Interrupts are similar to CPU <strong>traps</strong> (exceptions) that occur whenever a bad instruction occurs, like divide-by-zero.<h3 id=bulk-io><a href=#bulk-io aria-label="anchor link for bulk-io">###</a> Bulk IO</h3><p>Some devices need to dump a lot of data into the computer. We normally do this using a <strong>direct memory access (DMA)</strong>, which puts data from a device into the system's memory.<h2 id=hardware-protection><a href=#hardware-protection aria-label="anchor link for hardware-protection">##</a> Hardware Protection</h2><h3 id=cpu><a href=#cpu aria-label="anchor link for cpu">###</a> CPU</h3><p>It can be dangerous to any program to run every operation provided by the hardware. Therefore, most hardware uses <strong>dual-mode operation</strong> where there are two modes:<ul><li>Kernel Mode: Can access all instructions and all registers, privileged and unprivileged.<li>User Mode: Can only access unprivileged instructions and registers.</ul><p><em>Note:</em> Modern hardware often has more than just these two modes, but we won't talk about that here.<p>How do we switch modes? Normally, there are special instructions to switch to the appropriate mode. The kernel automatically switches back to user mode whenever it needs to, but how do you switch from user mode to kernel mode? The answer is <em>system calls</em> and <em>interrupts</em>.<p>Interrupts automatically switch to kernel mode at the start and switch to user mode (or whatever the old mode was) at the end.<p>System calls trigger a special trap on the CPU which goes into the appropriate system call.<p><em>How do we prevent user programs from preventing the OS to run?</em> The normal solution is to have a <strong>timer interrupt</strong>, which fires at a countdown to allow the OS to take control of the CPU eventually.<h3 id=i-o-device-controllers><a href=#i-o-device-controllers aria-label="anchor link for i-o-device-controllers">###</a> I/O & Device Controllers</h3><p>If you use port-mapped IO, it's easy to do protection. Just make those special instructions privileged.<p>If you use memory-mapped IO, you have to use memory protection.<h3 id=memory><a href=#memory aria-label="anchor link for memory">###</a> Memory</h3><p>Modern memory protection is very sophisticated. Here, we will use a simpler model that considers only a <strong>base register</strong> and a <strong>limit register</strong>. The base register stores the start of valid memory addresses. The limit register stores the size of the range. The hardware automatically checks these on all memory accesses.<p><em>Note:</em> These registers are privileged registers, meaning only the kernel can change them.<figure><img src=memory_protection.png><figcaption><p>Memory Protection</figcaption></figure><h1 id=os-architecture><a href=#os-architecture aria-label="anchor link for os-architecture">#</a> OS Architecture</h1><h2 id=microkernel><a href=#microkernel aria-label="anchor link for microkernel">##</a> Microkernel</h2><p>Microkernels take the position that <em>as little as possible</em> should be put in the kernel / run in kernel mode. This means that parts of the OS that normally run in the kernel (e.g. device drivers and file systems) instead run in userspace as processes.<p>Why should we do this?<ul><li>Easier to port to new machines (less code).<li>Less likely there will be bugs (less code).<li>Less code in kernel mode.<li>More extensible and modular. (Think about <code>rclone mount</code> and FUSE.)</ul><p>Why shouldn't we do this?<ul><li>Switching between kernel mode and user mode is expensive (interrupts).</ul><h2 id=monolithic-kernel><a href=#monolithic-kernel aria-label="anchor link for monolithic-kernel">##</a> Monolithic Kernel</h2><p>Monolithic kernels are the traditional design. It's where the kernel performs basically everything you'd expect an operating system to do.<p>Why should we do this?<ul><li>It has higher performance because it switches less between kernel and user mode.<li>Kernels grow very naturally.<li>You can work around the disadvantages pretty easily (see next section).</ul><p>Why shouldn't we do this?<ul><li>Less extensible and modular.<li>Harder to port to new machines.</ul><h3 id=kernel-modules><a href=#kernel-modules aria-label="anchor link for kernel-modules">###</a> Kernel Modules</h3><p>A class of dynamically loadable parts of the kernel. These can be run in user mode or kernel mode once loaded.<p>There are often specific <em>types</em> or <em>classes</em> of kernel modules, in a object-oriented manner. Modules implement interfaces specified by these types and that allows the kernel to load them.<p>In Linux, these are <code>.ko</code> (kernel object) files.<h2 id=boot-up><a href=#boot-up aria-label="anchor link for boot-up">##</a> Boot Up</h2><p>Operating systems can be huge. This means that they don't fit in a page of memory (basically a section of memory) and can't be trivially loaded by hardware. To mitigate this, we write small programs called <strong>bootloaders</strong>, which help load your operating system.<p>Here's a quick overview of the standard steps that occur whenever you turn on your computer. Different systems may have slight modifications, but the general process remains the same.<ol><li>Start running firmware at known address.<li>Load bootloader from secondary storage.<li>Bootloader copies kernel into memory ad begins execution.<li>Kernel initializes necessary data structures (e.g. interrupt vectors).<li>Kernel starts running init process (in user mode).</ol><p>More modern computers can use EFI stubs, which (can) completely replace bootloaders. However, we won't cover that here.<h1 id=inter-process-communication-ipc><a href=#inter-process-communication-ipc aria-label="anchor link for inter-process-communication-ipc">#</a> Inter-Process Communication (IPC)</h1><p>There are many ways to do IPC. The two standard models involve either <strong>direct sharing</strong> of memory (or other hardware) or <strong>message passing</strong>, which is a fancy way to say copying memory or other things.<h2 id=signals><a href=#signals aria-label="anchor link for signals">##</a> Signals</h2><p>The simplest is signals. They are mostly used to cancel, kill, or do something similar to the process. For example, <code>CTRL-C</code> sends <code>SIGINT</code> (interrupted), which tries to interrupt (and normally terminate) the process which received the signal.<p>This is hardly inter-process communication.<h2 id=message-passing><a href=#message-passing aria-label="anchor link for message-passing">##</a> Message Passing</h2><p><em>This is easier to get right, but (theoretically) less performant.</em><p>Processes have independent <em>mailboxes</em> that can hold data. Other processes can copy messages / data into your mailbox by going through the operating system. Only you can read your mailbox. No other process can read or write to your mailbox.<p>There are two main ways to do this. <em>Direct</em>, where you send directly to a process, or <em>indirect</em>, where you send through a named communication channel (e.g. named pipes).<p>The kernel may temporarily copy data into its own memory while it waits to deliver. This is called <strong>buffering</strong>.<h3 id=anonymous-pipes><a href=#anonymous-pipes aria-label="anchor link for anonymous-pipes">###</a> Anonymous Pipes</h3><ul><li><code>#include &lt;unistd.h></code><li><code>pipe(int fds[2])</code>: Create pipe with read end <code>0</code> and write end <code>1</code>.<ul><li>Use alphabetical order of read and write to remember the numbers. (Or just <code>man pipe(3)</code>.)</ul></ul><p>Pipes are queues of bytes that don't respect message boundaries. You give it a 2-array of ints and it gives you two file descriptors, a read end <code>0</code> and write end <code>1</code>.<p><em>Message boundaries?</em> Basically, if you write 2 bytes, 5 bytes, and 3 bytes to a pipe, the other end will see just 10 bytes.<p>POSIX defines <code>send</code> and <code>recv</code> syscalls that can send and receive messages on pipes. However, you can also use <code>read</code> and <code>write</code> syscalls, and you'll normally want to.<p>You normally use pipes by calling pipe, forking, and then having one process close one end and the other process close the other end (to prevent the OS from mistakenly thinking that someone is still using the pipe). When all write ends of the pipe is closed, from the child exiting or manually closing the pipe, the read end of the pipe receives an <code>EOF</code>.<p><em>Note:</em> If you max out the OS's buffer, the <code>write</code> syscall blocks.<h3 id=posix-message-queues><a href=#posix-message-queues aria-label="anchor link for posix-message-queues">###</a> POSIX Message Queues</h3><ul><li><code>#include &lt;mqueue.h></code><li><code>mq_open()</code>: Opens message queue. Give it a maximum message size and maximum number of messages to store.<li><code>mq_send()</code>: Sends message across queue.<li><code>mq_receive()</code>: Receives message from queue.<li><code>mq_close()</code>: Stop using the message queue.<li><code>mq_unlink()</code>: Destroys the message queue.</ul><p>Message queues are named queues of bytes that respect message boundaries.<p>Message queues are identifiable by a unique name (starting with <code>/</code>) across completely separate processes on the machine. They have their own completely different syscalls, unlike files and pipes, i.e. you can't use <code>read</code> or <code>write</code> on them.<p>Message queues can live longer than the process that makes them and can exist without any process making them. To remove them, you must call <code>mq_unlink</code> to destroy the message queue.<p>If you want to create the message queue, but one already exists, you get an error through <code>errno</code>. If you want to hook up to an existing message queue, but one doesn't exist, you get an error through error through <code>errno</code>.<h2 id=shared-memory><a href=#shared-memory aria-label="anchor link for shared-memory">##</a> Shared Memory</h2><ul><li><code>#include &lt;sys/shm.h></code><li><code>shmget()</code> (Shared Memory Get): Create a shared memory identifier (<code>int</code>) for a section shared memory. This memory is automatically zeroed.<li><code>shmat()</code> (Shared Memory Attach): Attach the shared memory to your address space, making it look like just another pointer. You can mark this memory as read-only (<code>SHM_RDONLY</code>), write-only, or read-write.<ul><li>You can specify a memory address to attach to, but you probably shouldn't. <code>shmdt()</code> (Shared Memory Detach): Remove the shared memory at the given pointer from your address space, but don't delete the shared memory.</ul><li><code>shmctl()</code> (Shared Memory Control): Delete (or otherwise control) the shared memory given by the shared memory identifier (<code>int</code>).</ul><p><em>This is hard to get right, but (theoretically) more performant.</em><p>There is a block of memory that both processes can read and write to. The operating system does very little control and trusts the programs to get it right.<p>This is accessed just like any other buffer because the OS maps the shared memory into your program's address space.<h2 id=blocking><a href=#blocking aria-label="anchor link for blocking">##</a> Blocking</h2><p>Whenever your program asks for something that the OS can't provide immediately, your process will <strong>block</strong>. (Assuming you're using a blocking system call.) When your process blocks, the OS puts your process into a waiting state, where it is not taking up (m)any resources. The OS will wake up your process when it gets what you asked for.<p>There are a few main blocking methods for IPC:<ul><li>Blocking Receive.<li>Non-blocking Receive.<li>Blocking Send.<li>Non-blocking Send.<li>Rendezvous: Blocking send and receive, with no buffer.</ul><h3 id=buffering><a href=#buffering aria-label="anchor link for buffering">###</a> Buffering</h3><p>Buffering is how we let non-blocking send work. The OS temporarily holds the data being shared while the receiver (ideally) catches up. This lets the sender not be slowed down.<h3 id=busy-waiting><a href=#busy-waiting aria-label="anchor link for busy-waiting">###</a> Busy Waiting</h3><p>Busy waiting is when your program takes as much CPU time as it can to just wait for things. <em>This is terrible.</em> It's basically a poor man's blocking.<h1 id=processes><a href=#processes aria-label="anchor link for processes">#</a> Processes</h1><p>There are a bunch of ways to handle parallelism / concurrency. We will cover processes right now.<p><strong>Processes</strong> are separate programs identified by a <strong>PID</strong> (process identifier). They have separate, protected memory and (can) run on separate CPUs. Processes can only interfere with each other in controlled ways (e.g. shared memory, signals, pipes). Processes also have the concept of privilege, which we'll cover later.<h2 id=syscalls><a href=#syscalls aria-label="anchor link for syscalls">##</a> Syscalls</h2><ul><li><code>fork</code>: Create a child process. Both the child and the parent run the same program with the same variables on the same line. On the parent <code>fork</code> returns the PID of the child. On the child, it returns 0.<li><code>wait</code>: Wait for a child to terminate. If given a PID, it waits for that specific child. If not given a PID, it just waits for any one child.<ul><li>Child termination forms a queue. Whenever you wait for a child, it pops that child off the queue, blocking if the queue is empty.</ul></ul><h2 id=memory-layout><a href=#memory-layout aria-label="anchor link for memory-layout">##</a> Memory Layout</h2><p>Processes has multiple sections of memory. They aren't really contiguous, but it's easy to visualize that way.<ul><li>Text: Immutable data, can be shared.<li>Data: Mutable data, should not be shared.<li>Stack: Local variables, return addresses.<li>Heap: Dynamically allocated memory.</ul><figure><img src=process_memory_layout.png><figcaption><p>Memory Layout</figcaption></figure><h2 id=process-organization><a href=#process-organization aria-label="anchor link for process-organization">##</a> Process Organization</h2><p>Process are normally organized into a <strong>process table</strong>, which is a list of all PCBs on the system. However, this is not very useful for scheduling.<p>To make scheduling more efficient, we keep scheduling queue(s), which is a list of PCBs for processes waiting. These queues are organized in various ways to make scheduling more efficient. Here's a few common ones.<ul><li>Ready Queue: Ready to run.<li>Device Queue: Waiting on a specific device.</ul><figure><img src=process_queues.png><figcaption><p>Process Queues</figcaption></figure><h3 id=time-sharing><a href=#time-sharing aria-label="anchor link for time-sharing">###</a> Time Sharing</h3><p>Whenever the OS does work, it may not switch back to the process that was previously running. The OS has a <strong>scheduler</strong> which it uses to determine what process to next run. They are incredibly complicated, so we will cover that later.<p>Switching between processes requires a <strong>context switch</strong>. A context switch is where the OS writes out all CPU registers (and other data) into memory and loads another process's saved data. It then executes the process whose data it just loaded.<p>This is great because it allows for processes to share! However, it is <em>very expensive</em> because processes keep track of a lot, for example:<ul><li>PID.<li>Copies of program counter and other CPU registers.<li>Memory bounds.<li>Accounting information.<li>Resources in use (e.g. open files).<li>Pointers for linking PCBs into various lists.</ul><p><em>Where does it store this info?</em> The OS stores the info in a <strong>process control block</strong>, which is a block of memory that the OS sets aside for process information.<h3 id=process-state><a href=#process-state aria-label="anchor link for process-state">###</a> Process State</h3><p>This isn't related to the process control block we talked about earlier. Instead this deals with <em>scheduling</em>.<p>This state indicates how the OS should treat the process during scheduling. The specific design may be different, but this is the standard.<ul><li>New: Created and can't be run yet.<li>Running: Executing instructions on a CPU.<li>Waiting: Process has requested I/O (or something else) and can't run until it's complete.<li>Ready: Process is runnable, but isn't on a CPU right now.<li>Terminated: Process has finished, still has a process ID, but isn't runnable.</ul><figure><img src=process_scheduling_states.png><figcaption><p>Process Scheduling States</figcaption></figure><h3 id=types-of-processes><a href=#types-of-processes aria-label="anchor link for types-of-processes">###</a> Types of Processes</h3><p>In terms of scheduling, it is useful to think about what the process spends most of its time doing. To help think about this we use a <em>CPU-IO burst cycle</em>, where programs switch between waiting on IO and using the CPU. <em>CPU bound processes</em> are programs that spend most of their time using the CPU. <em>IO bound processes</em> are programs that spend most of their time waiting for IO.<h3 id=process-tree><a href=#process-tree aria-label="anchor link for process-tree">###</a> Process Tree</h3><p>POSIX organizes processes as a tree, where processes have a parent and a child. The root is called the <strong>init</strong> process and is what runs initially and forks off every other process.<p>There are many models on how parents and children operate. They can share everything or share nothing. POSIX is somewhere in between. The child can be can run the same program as the parent (POSIX <code>fork</code>) or run a completely different program (Windows <code>CreateProcess</code>).<p><em>Note:</em> In POSIX, you use the <code>exec*</code> family of syscalls to replace the running process a different program. These system calls (if successful) never return.<p><em>What happens if the parent of a process dies?</em> In POSIX the child keeps running normally, but the OS sends you a signal that your parent died. In other systems, there is <em>cascading termination</em>, where the the OS kills the child (and its children).<h1 id=threads><a href=#threads aria-label="anchor link for threads">#</a> Threads</h1><p>Threads are like processes that are even more cooperative (but less so than coroutines). They share code, data, heap, and OS allocated resources (e.g. files). They have different CPU registers, positions in code, and stack.<figure><img src=thread_stacks.png><figcaption><p>Process Address Space Illustration</figcaption></figure><p>For performance reasons, we want to keep the per-thread state as little as possible to ensure that we can quickly switch between threads and start additional threads. In other words, we want to keep the <strong>thread control block</strong> tiny.<h2 id=posix-threads><a href=#posix-threads aria-label="anchor link for posix-threads">##</a> POSIX Threads</h2><ul><li><code>#include &lt;pthread.h></code><li><code>pthread_create</code>: Creates a new thread with a <em>given start routine</em>. This start routine is the lifetime of the program, like <code>main</code>. This can give the start routine some data.<li><code>pthread_join</code>: Wait for the given thread to return / exit. This can return data by filling in a pointer.<li><code>pthread_exit</code>: <code>exit()</code> but for a thread and returning abstract data.</ul><p>A start routine returns a <code>void *</code> and accepts a single <code>void *</code>. This is used for input and output. <em>Make sure that the argument and return have appropriate lifetimes.</em> You should be very careful about giving/returning data between threads on the stack. In general, you'll want to heap allocate the argument and return.<h2 id=java-threads><a href=#java-threads aria-label="anchor link for java-threads">##</a> Java Threads</h2><p><code>java.lang.Thread</code> is how Java represents Threads. There are many different ways to tell Java what to run in the thread. You can subclass <code>Thread</code> and override its <code>run</code> method. Then when you start instances of this subclass, they run your <code>run</code> method. You can pass <code>Thread</code> itself a <code>Runnable</code> (either a subclass or a lambda expression). Then running that instance of Thread will run that <code>Runnable</code>.<p>Java's threads don't run immediately. Instead you must call <code>start</code> on them for them to start. You can then <code>join</code> on them.<p>You can also make a <code>Thread</code> a daemon, by calling <code>setDaemon</code> to it. This must be set before you start the thread. This makes it run in the background (and thus run the JVM in the background). Like a daemon!<p>Java does directly support arguments too threads or returning from threads. Instead, you must subclass <code>Thread</code> to add the arguments via a constructor and return the arguments by having attributes on the thread that you look at at the end.<h2 id=user-level-threads-coroutines-green-threads><a href=#user-level-threads-coroutines-green-threads aria-label="anchor link for user-level-threads-coroutines-green-threads">##</a> User-Level Threads / Coroutines / Green Threads</h2><p>User-level threads are where you implement threading in userspace either by using a single thread / process or multiple without the kernel being aware that you are running multiple threads.<p>Using OS provided threads requires syscalls, which can be expensive. It's also not very portable because each OS tends to provide different interfaces. Because of this user-level threads can be quicker and more portable. Theoretically, user level threads can be even more cooperative too.<p>However, naive user-level threads have one huge issue: <strong>blocking</strong>. When a single one of your user-level threads makes a blocking syscall, the OS might block your entire thread, preventing all your other user-level threads to execute. There are many different ways to handle this. Go does this by creating a new blocking thread for each blocking syscall, so the kernel blocks that thread and not the one you were using.<h2 id=thread-pools><a href=#thread-pools aria-label="anchor link for thread-pools">##</a> Thread Pools</h2><p>A common way to get around the issues with context switching and the cost of creating threads is by creating a fixed size thread pool. You then split up your work not by threads but by units of work. Then the thread pool distributes the work among the finite number of threads.<h2 id=thread-cancellation><a href=#thread-cancellation aria-label="anchor link for thread-cancellation">##</a> Thread Cancellation</h2><p>How do you ask/tell a thread to die? If it dies at the wrong point, it might leave your data (which is shared by many threads) corrupted / inconsistent. There are two main ways.<ul><li>Asynchronous Cancellation: Force a thread to right now.<li>Deferred Cancellation: Create cancellation points, where the thread checks whether it should still be running. Then the thread terminates gracefully.</ul><h2 id=contention><a href=#contention aria-label="anchor link for contention">##</a> Contention</h2><p>Threads can either run only within their process or run against other processes. <em>Process contention scope</em> is where the thread only competes for time against other threads within its process. <em>System contention scope</em> is the thread competes for time against all other processes on the system.<p><em>Note:</em> <code>pthreads</code> let's us specify a thread's scope.<h1 id=cpu-scheduling><a href=#cpu-scheduling aria-label="anchor link for cpu-scheduling">#</a> CPU Scheduling</h1><p>When does the CPU scheduler run? If your scheduler does the last two, it's called <strong>preemptive</strong> because it can forcefully interrupt a process.<ul><li>Process terminates.<li>Process voluntarily yields the CPU.<li>Process blocks (syscall).<li>Another process finishes waiting (hardware interrupt). <em>Preemption</em>.<li>Process has had enough CPU time for now (timer interrupt). <em>Preemption</em>.</ul><p>Once we pick a process, we need to do <em>process dispatch</em> via a <strong>dispatcher</strong>. This installs saved context, switches modes, and updates memory bounds. The time it takes to do this is called <em>dispatch latency</em>.<p>A CPU scheduler wants to min-max the following. However, notice that <em>there is no globally optimal solutions for all cases</em>. For example, minimizing response time requires more context switches, causing more context switches and decreased throughput.<ul><li>CPU Utilization: How often is the CPU running a user process.<li>Throughput: How many processes (or CPU bursts) are finished per time unit.<li>Turnaround time: Time from when a job is submitted until its done.<ul><li>Normally not great for benchmarking scheduler performance.</ul><li>Waiting time: Time a process spend in the ready state.<li>Response time: Time from when a job becomes runnable to when it starts producing output.</ul><p><em>Note:</em> For measuring effectiveness, we normally measure waiting time, since that is due largely to the decisions of the scheduler.<h2 id=first-come-first-served-fcfs-scheduling><a href=#first-come-first-served-fcfs-scheduling aria-label="anchor link for first-come-first-served-fcfs-scheduling">##</a> First-Come, First-Served (FCFS) Scheduling</h2><p>This incredibly simple method of scheduling just runs processes in the order they arrive and runs them without preemption.<p>It is <em>fast</em>, taking constant time for inserting new processes and selecting new processes, and has reduced scheduling overhead by being non-preemptive.<p>FCFS schedulers exhibit the <em>convoy effect</em>, where you can have many processes that need short CPU bursts piled up behind a few that need a lot time.<h2 id=shortest-job-first-sjf-shortest-remaining-time-first-srtf-scheduling><a href=#shortest-job-first-sjf-shortest-remaining-time-first-srtf-scheduling aria-label="anchor link for shortest-job-first-sjf-shortest-remaining-time-first-srtf-scheduling">##</a> Shortest Job First (SJF) / Shortest Remaining Time First (SRTF) Scheduling</h2><p>SJF/SRTF is a provably optimal scheduler in terms of waiting time (for non-preemptive and preemptive schedulers respectively). What SJF does is look at the jobs' burst time and schedule the job with the shortest burst time first. What SRTF does is look at the jobs' burst time and schedule the job with the smallest remaining time first; when a new job arrives, it interrupts the currently running one to put the one with the smallest remaining time back in.<p>They are virtually identical, but SJF is non-preemptive, which SRTF is preemptive.<p>Even though these are theoretically optimal for average waiting time, in practice they rely on us knowing (approximately) the running time of a process, which is <em>unrealistic</em>, and there's also a risk of <em>starvation</em>.<h3 id=approximations-of-running-time><a href=#approximations-of-running-time aria-label="anchor link for approximations-of-running-time">###</a> Approximations of Running Time</h3><p>There are several ways to estimate the time a CPU burst needs. Most of them involve statistical techniques using the process's previous CPU burst times.<p>One way is a simple <em>average</em> of your previous bursts. Another way is an <em>exponential average</em> of your previous bursts (weight the previous ones exponentially less).<p>The math for exponential average, where <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D70F"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>τ</mi></math></mjx-assistive-mml></mjx-container> is the estimate burst time, <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D44F"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>b</mi></math></mjx-assistive-mml></mjx-container> is the actual burst time, and <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D6FC"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>α</mi></math></mjx-assistive-mml></mjx-container> is some arbitrary constant (1 is more reactive, 0 is more smoothing)<p><mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative display=true><mjx-math aria-hidden=true class=MJX-TEX display=true style=margin-left:0;margin-right:0><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D70F"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em;margin-left:-.08em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi><mjx-mo class=mjx-n><mjx-c class=mjx-c2B></mjx-c></mjx-mo><mjx-mn class=mjx-n><mjx-c class=mjx-c31></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=mjx-n space=4><mjx-c class=mjx-c3D></mjx-c></mjx-mo><mjx-mi class=mjx-i space=4><mjx-c class="TEX-I mjx-c1D6FC"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D44F"></mjx-c></mjx-mi><mjx-mo class=mjx-n space=3><mjx-c class=mjx-c2B></mjx-c></mjx-mo><mjx-mo class=mjx-n space=3><mjx-c class=mjx-c28></mjx-c></mjx-mo><mjx-mn class=mjx-n><mjx-c class=mjx-c31></mjx-c></mjx-mn><mjx-mo class=mjx-n space=3><mjx-c class=mjx-c2212></mjx-c></mjx-mo><mjx-mi class=mjx-i space=3><mjx-c class="TEX-I mjx-c1D6FC"></mjx-c></mjx-mi><mjx-mo class=mjx-n><mjx-c class=mjx-c29></mjx-c></mjx-mo><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D70F"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em;margin-left:-.08em><mjx-mi class=mjx-i size=s><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml display=block unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML display=block><msub><mi>τ</mi><mrow data-mjx-texclass=ORD><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>α</mi><mi>b</mi><mo>+</mo><mo stretchy=false>(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy=false>)</mo><msub><mi>τ</mi><mi>n</mi></msub></math></mjx-assistive-mml></mjx-container><h2 id=priority-scheduling><a href=#priority-scheduling aria-label="anchor link for priority-scheduling">##</a> Priority Scheduling</h2><p>Priority scheduling gives every process a priority number. We then schedule things with a higher priority first. In pure priority scheduling (what we'll do in this class), we only look the priority of a process, ignoring time.<p>Priority scheduling is not inherently preemptive or non-preemptive.<p><em>Note:</em> In this class, we consider smaller priority numbers to be higher priority to match POSIX.<p>In Linux, you can change the priority of your process by using the <code>nice</code> command. It ranges from -20 to 19, but regular users can't set negative priorities (which are higher).<h2 id=round-robin-rr-scheduling><a href=#round-robin-rr-scheduling aria-label="anchor link for round-robin-rr-scheduling">##</a> Round Robin (RR) Scheduling</h2><p>Here, we chunk up time into <em>time quantums</em> (normally between 10-100ms) and then distribute these in a round-robin fashion. You maintain a ready queue of processes. Whenever you start a new time quantum, your program picks the first process of the ready queue. When you end a time quantum, the OS preempts the process, putting it on the back of the ready queue, and taking the next one off the queue. If your program blocks before its time quantum is done, the program just goes to the next time quantum early.<p><em>Why is this good?</em> This bounds wait time to <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mo class=mjx-n><mjx-c class=mjx-c28></mjx-c></mjx-mo><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi><mjx-mo class=mjx-n space=3><mjx-c class=mjx-c2212></mjx-c></mjx-mo><mjx-mn class=mjx-n space=3><mjx-c class=mjx-c31></mjx-c></mjx-mn><mjx-mo class=mjx-n><mjx-c class=mjx-c29></mjx-c></mjx-mo><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45E"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mo stretchy=false>(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy=false>)</mo><mi>q</mi></math></mjx-assistive-mml></mjx-container>. It makes CPU time very predictable, because you always get <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mn class=mjx-n><mjx-c class=mjx-c31></mjx-c></mjx-mn><mjx-texatom texclass=ORD><mjx-mo class=mjx-n><mjx-c class=mjx-c2F></mjx-c></mjx-mo></mjx-texatom><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mn>1</mn><mrow data-mjx-texclass=ORD><mo>/</mo></mrow><mi>n</mi></math></mjx-assistive-mml></mjx-container> time, so it's like you processor is just <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>n</mi></math></mjx-assistive-mml></mjx-container> times lower. This means turnaround time is longer but response time is quicker.<p><em>How do you pick a good quantum?</em> There's a trade-off between response time and overhead. The smaller your time quantum, the faster your response time but the more overhead (from context switches). The larger your time quantum, the slower your response time.<h2 id=real-time-scheduling><a href=#real-time-scheduling aria-label="anchor link for real-time-scheduling">##</a> Real-Time Scheduling</h2><p>Real-time scheduling is when the scheduling of your program is a critical part of its correctness. As in, your operating system must guarantee that a process can run within say 5ms and let it run for say 10ms. Failure to do so would be a critical failure. <em>These are called hard real time operating systems / schedulers.</em><p>Most general purpose operating system's do not meet hard real-time scheduling requirements, but they do try to meet soft real-time scheduling requirements, such as those required for multi-media programs. Soft real-time scheduling is where you're expected to respond quickly and keep vaguely in real time, but the value doesn't drop to zero if you fail to meet them.<p><em>This is done by designating processes as real time processes, giving them higher priority.</em><p>In Linux, regular users can't make their processes real time. However, to do this, you must use <code>chrt</code>.<h2 id=multilevel-queue-scheduling><a href=#multilevel-queue-scheduling aria-label="anchor link for multilevel-queue-scheduling">##</a> Multilevel Queue Scheduling</h2><p>The idea of multilevel queue scheduling is that some categories of processes are more important. For example, batch jobs you want to run FCFS to minimize waiting time, but interactive jobs you want to run with RR to response time.<p>How do you determine which level queue a process should go in? You could have processes tell you, but processes can change and different OSes implement scheduling differently, so it's hard to do cross platform. We normally schedule this using <em>feedback</em>, where a process moves between the queues.<p>Feedback is based on aging or process behavior.<p>For example (aging), new processes may go into a RR queue with 8ms. Then going to 16ms RR queue. Then going to a FCFS queue. The idea is you bet that the process won't take much time, giving it a bit more time until it's done. This would give you a good balance of waiting time and response time.<h2 id=multi-processor-cpu><a href=#multi-processor-cpu aria-label="anchor link for multi-processor-cpu">##</a> Multi-Processor CPU</h2><p><em>Asymmetric multi-processing</em> is where we treat specific cores differently. This is more common in <em>heterogeneous</em> multi-processor systems, where you have for example a specific core for sound processing. <em>Symmetric multi-processing</em> (SMP) is where we treat all cores equally. This is very common in <em>homogeneous</em> multi-processor systems.<p>Processes tend to run faster if they are repeatedly put on the same processor because of the cache. <em>Soft processor affinity</em> is where the OS tries to schedule processes on the same processors, but is willing to not. <em>Hard processor affinity</em> is where the OS mandates the processes are on the same processors; this is normally done via request.<h2 id=linux-scheduler><a href=#linux-scheduler aria-label="anchor link for linux-scheduler">##</a> Linux Scheduler</h2><p>Each CPU core has its own ready queue that it stores in a red-black tree sorted by virtual runtime. It is a work-stealing scheduler, meaning that a CPU can steal work from other cores if it runs out of work.<p><em>Virtual run time</em> is the runtime of the program scaled by its niceness. Less nice processes get charged less for their time. More nice processes get charged more.<p>Per POSIX, Linux has 140 different priorities, with the first 100 for real-time processes.<h1 id=synchronization><a href=#synchronization aria-label="anchor link for synchronization">#</a> Synchronization</h1><p>A <strong>race condition</strong> is when the correctness of your program depends on the order of execution of multiple threads/processes. This normally occurs because of non-atomic operations / memory getting changed unexpectedly, especially with shared memory.<p><em>Note:</em> Having multiple threads read a piece of memory is okay. However, having one thread read/write a piece of memory and any other thread read (or write) that same memory is incorrect.<h2 id=classic-synchronization-problems><a href=#classic-synchronization-problems aria-label="anchor link for classic-synchronization-problems">##</a> Classic Synchronization Problems</h2><p>There are a few standard problems that basically all computer scientists learn about.<h3 id=bounded-buffer-thread-safe-queue><a href=#bounded-buffer-thread-safe-queue aria-label="anchor link for bounded-buffer-thread-safe-queue">###</a> Bounded Buffer / Thread-safe Queue</h3><p>A bounded buffer is a multi-producer, multi-consumer queue of elements. If you want to solve this just using semaphores, it would look something like this<pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-c z-source">sem emptyCount <span class="z-c z-keyword z-operator z-assignment">=</span> BUFFER_SIZE<span class="z-c z-punctuation z-terminator">;</span>
sem fullCount <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-terminator">;</span>
sem lock <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-terminator">;</span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">producer</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    Item it <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">makeSomething</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">acquire</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">emptyCount</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span> <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> emptyCount--
</span>    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">acquire</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">lock</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
    buffer<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>first <span class="z-c z-keyword z-operator z-arithmetic">+</span> num<span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-keyword z-operator z-arithmetic">%</span> BUFFER_SIZE<span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> it<span class="z-c z-punctuation z-terminator">;</span>
    num<span class="z-c z-keyword z-operator z-arithmetic">++</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">release</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">lock</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">release</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">fullCount</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span> <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> fullCount++
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">consumer</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">acquire</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">fullCount</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span> <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> fullCount--
</span>    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">acquire</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">lock</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
    Item it <span class="z-c z-keyword z-operator z-assignment">=</span> buffer<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span>first<span class="z-c z-punctuation z-section z-end z-brackets">]</span></span><span class="z-c z-punctuation z-terminator">;</span>
    first <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>first <span class="z-c z-keyword z-operator z-arithmetic">+</span> <span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-keyword z-operator z-arithmetic">%</span> BUFFER_SIZE<span class="z-c z-punctuation z-terminator">;</span>
    num<span class="z-c z-keyword z-operator z-arithmetic">--</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">release</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">lock</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">release</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">emptyCount</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span> <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> emptyCount++
</span>    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">consumeItem</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">it</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
</span></code></pre><h3 id=dining-philosophers-problem><a href=#dining-philosophers-problem aria-label="anchor link for dining-philosophers-problem">###</a> Dining Philosophers Problem</h3><p>Imagine you have multiple philosophers sitting at a table. They each have a plate in front of them. Between each plate there is a single chopstick. All philosophers are sitting there thinking, looking up. Whenever they get hungry, they grab the chopstick on their left and then later on their right.<p><em>How do you prevent the Philosopher's from deadlocking?</em> They deadlock when they all grab the chopstick on their left and thus no one can grab the on on their right.<p>Here's a quick list of the solutions, along with comments on them.<ul><li>Keep an extra chopstick around: More resources aren't always available.<li>Only one at a time can eat: Horribly slow.<li>Only pick up a chopstick if you can get both: Might have starvation. Suppose the philosopher to your right starts eating and then the one to your left starts eating, alternating forever. You never get to eat.<li>Break the symmetry, make some pick left first and others right first: Works well and is reusable!</ul><h3 id=readers-and-writers-problem><a href=#readers-and-writers-problem aria-label="anchor link for readers-and-writers-problem">###</a> Readers and Writers Problem</h3><p>This issue is similar to the critical section problem solved with mutual exclusion. However, solving this problem allows for more concurrency.<p>Normally, it is safe for multiple threads to read concurrently. However, it is not safe for one thread to write while there is another thread looking/writing the memory. Formally, where <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D464"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>n</mi><mi>w</mi></math></mjx-assistive-mml></mjx-container> is the number of writers and <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45F"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>n</mi><mi>r</mi></math></mjx-assistive-mml></mjx-container> is the number of readers, it is safe if (<mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45F"></mjx-c></mjx-mi><mjx-mo class=mjx-n space=4><mjx-c class=mjx-c3D></mjx-c></mjx-mo><mjx-mn class=mjx-n space=4><mjx-c class=mjx-c30></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>n</mi><mi>r</mi><mo>=</mo><mn>0</mn></math></mjx-assistive-mml></mjx-container> or <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D464"></mjx-c></mjx-mi><mjx-mo class=mjx-n space=4><mjx-c class=mjx-c3D></mjx-c></mjx-mo><mjx-mn class=mjx-n space=4><mjx-c class=mjx-c30></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>n</mi><mi>w</mi><mo>=</mo><mn>0</mn></math></mjx-assistive-mml></mjx-container>) and <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D464"></mjx-c></mjx-mi><mjx-mo class=mjx-n space=4><mjx-c class=mjx-c2264></mjx-c></mjx-mo><mjx-mn class=mjx-n space=4><mjx-c class=mjx-c31></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>n</mi><mi>w</mi><mo>≤</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container>.<p>We now implement this using monitor pseudo-code. This implementation has an issue with <em>starvation</em>, where writers may starve because readers can get in if other people are reading but a writer cannot. We could fix it, but we don't here.<pre class="z-code language-nohighlight" data-lang=nohighlight><code class=language-nohighlight data-lang=nohighlight><span class="z-plain z-text">monitor ReadersWriters {
  // Number of readers
  int nr = 0;
  // Number of writers
  int nw = 0;

  // Okay to read?
  Condition readable;
  // Okay to write?
  Condition writable;

  void lockReading() {
    while (nw > 0) {
      readable.wait();
    }
    nr++;
    // Other readers waiting should know that
    // it's safe to read. We do this because our
    // pseudo-code doesn't use broadcast
    readable.signal();
  }
  void unlockReading() {
    nr--;
    if (nr == 0) {
      // Don't broadcast because we only want to
      // wake up one writer.
      writable.signal();
    }
  }

  void lockWriting() {
    while (nr > 0 || nw > 0) {
      writable.wait();
    }
    nw++;
  }
  void unlockWriting() {
    nw--;

    // Wake a reader and a writer and let them
    // race.
    writable.signal();
    // We don't have broadcast >:(
    readable.signal();
  }
}
</span></code></pre><h3 id=priority-inversion><a href=#priority-inversion aria-label="anchor link for priority-inversion">###</a> Priority Inversion</h3><p>Synchronization mechanisms can interfere with priority issues. Suppose you have some cheap low-priority job that acquires a lock. Then you have a medium-priority job that is very CPU intensive. Normally, the medium-priority job will run a lot more than the low priority job. However, suppose then a high-priority job comes along and tries to get the same lock the low-priority job got. Now the high-priority job is blocked by a low-priority job and will have to wait a long time!<p><em>How do we fix this?</em> Normally, RTOS's (and possible normal OS's) will have the option for <em>priority-inheritance</em>, where the low-priority job will be promoted to the same level as the high-priority job while it is blocking that high-priority job.<h2 id=synchronization-mechanisms><a href=#synchronization-mechanisms aria-label="anchor link for synchronization-mechanisms">##</a> Synchronization Mechanisms</h2><p>Most of the times with memory, you have a <em>critical section problem</em>, where you only have a single section (or a few) interacting with shared memory. To keep this memory safe from corruption, we put up guards around this critical section using various <strong>synchronization mechanisms</strong> (a.k.a. primitives).<p><em>Note:</em> Ideally these critical sections are small and quick.<p>A good solution to the critical section problem should have the following properties:<ul><li>Mutual Exclusion: We only want one thread in the critical section at a time.<li>Progress: Progress will always be made, you never wait forever.<li>Bounded Waiting: Can't starve a thread.</ul><h3 id=naive-synchronization><a href=#naive-synchronization aria-label="anchor link for naive-synchronization">###</a> Naive Synchronization</h3><p>This example may look good, but it will allow multiple threads in if they both get past the while loop simultaneously and then set that they are inside.<pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-c z-source"><span class="z-c z-storage z-type">bool</span> inside<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">2</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-separator">,</span> <span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span><span class="z-c z-punctuation z-terminator">;</span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread0</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>inside<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span>
    inside<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    inside<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread1</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>inside<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span>
    inside<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    inside<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
</span></code></pre><p>This example solves the problem, but has the issue of deadlock, where they both say they want in and then wait for the other guy to finish.<pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-c z-source"><span class="z-c z-storage z-type">bool</span> wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">2</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-separator">,</span> <span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span><span class="z-c z-punctuation z-terminator">;</span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread0</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread1</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
</span></code></pre><p>We could try to resolve it by taking turns (strict alternation) and it works, but its generally a bad idea because its possible for the threads to not want to go through the critical section the same number of times and its also wasteful.<pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-c z-source"><span class="z-c z-storage z-type">int</span> turn <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-terminator">;</span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread0</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>turn <span class="z-c z-keyword z-operator z-comparison">==</span> <span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    turn <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread1</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>turn <span class="z-c z-keyword z-operator z-comparison">==</span> <span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    turn <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
</span></code></pre><h3 id=peterson-s-algorithm><a href=#peterson-s-algorithm aria-label="anchor link for peterson-s-algorithm">###</a> Peterson's Algorithm</h3><p>Combining strict alternation with <code>wantIn</code>. Essentially, it says "I want in, but I'll let you go first if its your turn." Most of the time turn isn't even looked at.<pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-c z-source"><span class="z-c z-storage z-type">int</span> turn <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-terminator">;</span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread0</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-terminator">;</span>
    turn <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-terminator">;</span> <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> no, after you
</span>    <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-arithmetic">&&</span> turn <span class="z-c z-keyword z-operator z-comparison">==</span> <span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread1</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-terminator">;</span>
    turn <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-terminator">;</span> <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> no, after you
</span>    <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-arithmetic">&&</span> turn <span class="z-c z-keyword z-operator z-comparison">==</span> <span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    wantIn<span class="z-c z-meta z-brackets"><span class="z-c z-punctuation z-section z-begin z-brackets">[</span><span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-section z-end z-brackets">]</span></span> <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-language">false</span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
</span></code></pre><p><em>Note:</em> This may not actually work anymore, since modern processors do not guarantee that memory writes occur in the same order appeared. This is because of performance. Because of this, we have various synchronization primitives.<h3 id=counting-semaphores><a href=#counting-semaphores aria-label="anchor link for counting-semaphores">###</a> Counting Semaphores</h3><p>Counting semaphores are conceptually counts of the number of threads that can be in the critical section at once. They can be naively implemented the following. However, most of the time semaphores are implemented by the OS and are much more efficient (e.g. they don't busy wait).<pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-c z-source"><span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">acquire</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">sem s</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span> <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>s <span class="z-c z-keyword z-operator z-comparison">&lt;=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">0</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span><span class="z-c z-punctuation z-section z-end z-block">}</span></span>
  s<span class="z-c z-keyword z-operator z-arithmetic">--</span><span class="z-c z-punctuation z-terminator">;</span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">release</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">sem s</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  s<span class="z-c z-keyword z-operator z-arithmetic">++</span><span class="z-c z-punctuation z-terminator">;</span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
</span></code></pre><p>Here's how a semaphore would apply to our previous problem.<pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-c z-source">sem s <span class="z-c z-keyword z-operator z-assignment">=</span> <span class="z-c z-constant z-decimal z-integer z-numeric">1</span><span class="z-c z-punctuation z-terminator">;</span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread0</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">acquire</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">s</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">release</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">s</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">someThread1</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
  <span class="z-c z-keyword z-control">while</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span><span class="z-c z-constant z-language">true</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span> <span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span>
    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">acquire</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">s</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> critical section
</span>    <span class="z-c z-meta z-function-call"><span class="z-c z-function z-variable">release</span><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group">s</span></span><span class="z-c z-meta z-function-call"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-punctuation z-terminator">;</span>
    <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> remainder section
</span>  <span class="z-c z-punctuation z-section z-end z-block">}</span></span>
<span class="z-c z-punctuation z-section z-end z-block">}</span></span>
</span></code></pre><p><em>Note:</em> In this class, we'll say you can't start semaphores at negative values. It varies across libraries.<h3 id=binary-semaphore-mutex><a href=#binary-semaphore-mutex aria-label="anchor link for binary-semaphore-mutex">###</a> Binary Semaphore / Mutex</h3><p>Binary semaphores and mutexes are similar and perform basically the exact same operation. Instead of having a count that gets decremented and incremented by <code>acquire</code> and <code>release</code>, you just have a boolean flag that either <code>lock</code>s or <code>unlock</code>s.<p>They cover the most common pattern of using a counting semaphore, where you just want to have a single thread in a critical section.<h3 id=atomic-operations-spinlocks><a href=#atomic-operations-spinlocks aria-label="anchor link for atomic-operations-spinlocks">###</a> Atomic Operations / Spinlocks</h3><p>Modern processors provide <strong>atomic operations</strong> and most libraries provider ways to access these ergonomically. Atomic operations are operations that will always run together and never have <em>any</em> instruction go in-between them and interfere / change things.<p>Most atomic operations have the mnemonic of <em>test and set</em>, where you do some check and then conditionally change the value.<p>Atomic operations can be used to implement <strong>spin-locks</strong>. Spin-locks are essentially good busy waiting, using atomic operations and normally yield after spinning for a bit.<h2 id=monitors><a href=#monitors aria-label="anchor link for monitors">##</a> Monitors</h2><p><em>Note:</em> Monitors with condition variables are provably equivalent to semaphores.<p>Monitors are easy ways to wrap functions in locks. They ensure that anyone that wants to call this function first acquires the appropriate lock and releases it when they're done.<p>This prevents us from waiting within a monitor. For example, for the bounded buffer problem, we try to add things through the monitor and no one else has called it. How do we show we should also block?<p>The trick is <strong>condition variables</strong>. Condition variables are basically events that we can either <strong>signal</strong> as happening or <strong>wait</strong> to happen. When you wait on a condition variable, you give up your lock on the monitor and go into a queue of people waiting on that condition variable / event.<p><em>Note:</em> Signaling on a condition variable where no one is waiting has no effect.<p>There are two different <strong>policies</strong> for how signaling works.<ul><li>Signal and Wait: Whenever you signal, you wait for the thread you signaled to start and finish.<li>Signal and Continue: Whenever you signal, you continue running until you're done with the monitor. The one you woke up gets to run once you're done.<ul><li>Most common!</ul></ul><p>We use C/Java like syntax for monitors.<pre class="z-code language-nohighlight" data-lang=nohighlight><code class=language-nohighlight data-lang=nohighlight><span class="z-plain z-text">monitor Maximizer {
  // In some languages hasValC and hasVal might
  // be able to be in the same
  condition hasValC;
  bool hasVal = false;

  int largest;
  void submitValue(int val) {
    while (!hasVal || val > largest) {
      largest = val;
    }
    // this only wakes a single thread / waiter
    hasValC.signal();
    hasVal = true;
  }
  int getLargest() {
    if (!hasVal) {
      hasValC.wait();
      // tell anyone else waiting to wake up
      hasValC.signal();
    }
    return largest;
  }
}
</span></code></pre><pre class="z-code language-nohighlight" data-lang=nohighlight><code class=language-nohighlight data-lang=nohighlight><span class="z-plain z-text">// Synchronized stack
monitor IntStack {
  condition nonEmpty;
  Stack&lt;Integer> stack = new Stack&lt;>();
  int popInt() {
    // Must be while because suppose you have
    // three threads A, B, C. B is waiting for
    // something to be pushed. A just pushed
    // something. Then C came in and popped
    // after A pushed, without blocking. B then,
    // having been woke up by A, wakes up and
    // would try to pop from an empty stack
    // (because C stole its element) if there
    // was not a while loop.
    while (stack.size() &lt;= 0) {
      nonEmpty.wait();
    }
    return stack.pop();
  }
  void pushInt(int v) {
    stack.push(v);
    nonEmpty.signal();
  }
}
</span></code></pre><h2 id=apis><a href=#apis aria-label="anchor link for apis">##</a> APIs</h2><h3 id=semaphore-h><a href=#semaphore-h aria-label="anchor link for semaphore-h">###</a> <code>semaphore.h</code></h3><p><code>semaphore.h</code> is the POSIX semaphore library. It doesn't work on Mac sadly, although they do compile, so that's cool. There are both named semaphores <code>sem_open</code> (return <code>sem_t *</code>) and anonymous semaphores <code>sem_init</code> (return <code>sem_t</code>). You must provide them initial values.<ul><li>Type: <code>sem_t</code>.<li>Acquire: <code>sem_wait</code>.<li>Release: <code>sem_post</code>.</ul><h3 id=pthread-h><a href=#pthread-h aria-label="anchor link for pthread-h">###</a> <code>pthread.h</code></h3><p>Monitors are not primitives, but you can implement them via structs and functions.<ul><li>Monitor: C has no monitor primitives. Instead you use mutexes.<ul><li>Initialize: <code>pthread_mutex_init(pthread_mutex_t *)</code> or <code>PTHREAD_MUTEX_INITIALIZE</code>.<li>Enter Monitor: <code>pthread_mutext_lock(pthread_mutext_t *)</code>.<li>Leave Monitor: <code>pthread_mutext_unlock(pthread_mutext_t *)</code>.</ul><li>Condition Variables: Luckily, condition variables are primitives, but they aren't restricted. This uses <strong>signal and continue</strong> semantics.<ul><li>Initialize: <code>pthread_cond_init(pthread_cond_t *)</code> or <code>PTHREAD_COND_INITIALIZE</code>.<li>Wait: <code>pthread_cond_wait(pthread_cond_t *, pthread_mutex_t *)</code>.<li>Signal: <code>pthread_cond_signal(pthread_cond_t *)</code>.<li>Signal Everyone: <code>pthread_cond_broadcast(pthread_cond_t *)</code>.</ul></ul><h3 id=java-s-synchronized-keyword><a href=#java-s-synchronized-keyword aria-label="anchor link for java-s-synchronized-keyword">###</a> Java's <code>synchronized</code> Keyword</h3><p>In Java, every object can be used as a lock and any method can be a lock. The threads waiting for a lock on synchronized method is called the <strong>entry set</strong> and the JVM does not guarantee any order. The threads waiting for an object is called the <strong>wait set</strong> and the JVM does not guarantee any order.<p><em>Note:</em> Even if you treat an object as a lock, other people may not. You should thus prefer using synchronized methods over using synchronized blocks on an individual object instance.<ul><li>Monitor: You just put <code>synchronized</code> in a method's declaration and automatically you acquire the lock before you enter and automatically release the lock before you exit.<ul><li>All <code>synchronized</code> methods on an object exclude each other. Meaning if you have <code>synchronized a()</code> and <code>synchronized b()</code>, you can't call both <code>a</code> and <code>b</code> at the same time.</ul><li>Condition Variables: You call the appropriate methods on <em>any</em> <code>Object</code>. This uses <strong>signal and continue</strong> semantics.<ul><li>Wait: <code>this.wait()</code>. This gives up the lock on <code>this</code>. <em>This is why you don't want to create other objects just to wait on them</em>, otherwise you wouldn't give up the lock on <code>this</code>.<li>Signal: <code>this.notify()</code>. Just moves someone from the wait set and into the entry set.<li>Signal Everyone: <code>this.notifyAll()</code>. Moves everyone from the wait set and into the entry set. This is more useful, since Java doesn't give us arbitrarily many condition variables. You just use <code>while</code> loops normally.</ul></ul><h1 id=deadlock><a href=#deadlock aria-label="anchor link for deadlock">#</a> Deadlock</h1><p>A deadlock is a synchronization issue where you are prevented from making process. Formally, deadlock is when you have a set <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D446"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>S</mi></math></mjx-assistive-mml></mjx-container> of processes where every process in <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D446"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>S</mi></math></mjx-assistive-mml></mjx-container> is waiting to acquire a resource where the desired resource is held by another process in <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D446"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>S</mi></math></mjx-assistive-mml></mjx-container>.<p>We can help ourselves understand the issue using a <strong>resource allocation graph</strong>, where you use circles for processes and blocks for resources containing dots for an instance of that resource. <strong>Request edges</strong> are directed arrows from processes to resource blocks show desire (but not ownership) for instance. <strong>Assignment edges</strong> are directed arrows from resource dots to processes shows ownership.<p>Processes with request edges are generally <em>blocked</em> by the OS and the OS assigns resources to processes.<p>There are a few main ways of dealing with deadlock:<ul><li>Prevent: Design for no deadlock.<li>Detect: Watch for deadlocks and do something to recover from deadlocks.<li>Avoid: Regulate process behavior so that they never reach deadlock, even though they theoretically good.<ul><li>Think moves ahead in the game.</ul><li>Ignore: Just hope for the best and let users deal with it! :)<ul><li>What most OSes use! This is to avoid unnecessary overhead.</ul></ul><h2 id=deadlock-prevention><a href=#deadlock-prevention aria-label="anchor link for deadlock-prevention">##</a> Deadlock Prevention</h2><p>There are four <em>necessary conditions</em> for a deadlock to occur. If you don't have all of these, you can't have a deadlock.<ul><li>Mutual Exclusion: Some resources can only be used by one process at a time.<li>Hold and Wait: Processes can have some resources and ask for more.<li>No Preemption: OS gets a resource back only when a process is done with it.<li>Circular Wait: It's possible to build a cycle of processes.</ul><p>Which condition can we prevent/remove?<ul><li>Mutual Exclusion: The operating system generally doesn't use mutual exclusion of most resources. However, this isn't universally possible (e.g. for mutexes), so mutual exclusion is impractical to remove.<li>Hold and Wait: We could mandate that every process must request all resources at startup. However, this yields reduced resource utilization and possible starvation (because processes wait on things they hardly need or on things that others hardly need), so it is not very practical globally.<ul><li>Remember the "you must pick up both chopsticks at once" for the dining philosophers problem.</ul><li>Preemption: This actually isn't too bad and is done all over the place (e.g. in the CPU, in memory). The issue is that we have to remember the state where you left, but this isn't always possible because otherwise you might reach an invalid state or waste time.<li>Circular Wait: This is the most practical way to implement deadlock prevention. You do this by imposing a total ordering on all resource types, meaning you must ask for a lower number resource before you ask for a higher number resource. You can do this within your own processes, but it isn't always practical for the OS for the same reason as hold and wait.<ul><li>This let's you choose the priority of the resource!</ul></ul><h2 id=deadlock-detection><a href=#deadlock-detection aria-label="anchor link for deadlock-detection">##</a> Deadlock Detection</h2><p>Unlike deadlock prevention, this does not prevent limit the ability of programs or limit resource utilization (until deadlock).<p>The pseudo-code for detecting deadlock is as follows:<pre class="z-code language-nohighlight" data-lang=nohighlight><code class=language-nohighlight data-lang=nohighlight><span class="z-plain z-text">While there's some process P where all of P's
outstanding requests can be satisfied:
  Remove all request and assignment edges for P
  (to pretent P finishes without asking for
  more).
If any processes remain:
  You have a deadlock.
  Find cycle(s); the processes among them are
  responsible for the deadlock.
</span></code></pre><p>Now, how do we resolve the deadlock? We have to kill a process. We could kill a resource hog, which frees up a bunch of resources, but potentially wastes a bunch of time; it also means that resource hogs tend to starve. We could kill the youngest process, which means we waste less energy.<p><em>Why don't OSes use this?</em> It involves forcible killing of a process, meaning we wasted a bunch of time by killing an unfinished process. Also, this normally causes garbage-collector type pauses on our entire system, which isn't great.<h2 id=deadlock-avoidance><a href=#deadlock-avoidance aria-label="anchor link for deadlock-avoidance">##</a> Deadlock Avoidance</h2><p><em>Note:</em> This is mostly safe state detection.<p>If done right, this doesn't limit the ability of programs, prevents deadlocks entirely, and can only somewhat limit resource utilization.<p>Often, this requires us to know the future (i.e. what the processes might request). Practically this is done by having the OS mandate that all processes stake <strong>claims</strong> on certain resources, meaning they may want to acquire them sometime in the future. If a process doesn't have a claim on a resource, it can't get an instance of it. Claims are denoted with dashed arrows on the resource allocation graph. <em>Note:</em> Claims are often denoted with a certain multiplicity.<p>This requires us define <strong>safe states</strong> and <strong>unsafe states</strong>. Safe states are where it is impossible to reach deadlock. Unsafe states are where it is possible to reach deadlock.<p>When now run a <strong>safe state detector</strong> whenever a request for a resource is made. This detector ensures that granting the resource keeps us in a safe state. The algorithm for the detector is called the <strong>banker's algorithm</strong>. It goes like this<ul><li>Pretend like you granted the resource.<li>Turn all claim edges into request edges.<li>Run the deadlock detection algorithm.</ul><h1 id=memory-1><a href=#memory-1 aria-label="anchor link for memory-1">#</a> Memory</h1><p>We will think about memory as a <em>linearly addressable array of words</em> (smallest addressable unit). Recall memory protection!<p>For simplicity in this class, we pretend as though memory is split into two sections: one for the resident operating system and the other for user programs. We will also pretend as that memory is allocated contiguously we with a base (start of memory) and limit (end of memory) registers.<p>We want our programs to not be dependent on where specifically they are loaded. How do we do this? We do this with specific <strong>address bindings</strong>. Address bindings describe how we choose the physical memory space the program will run in. There are a few main ways:<ul><li>Compile Time: The compiler chooses the address when it builds. That is, the compiler generates <em>absolute code</em>.<li>Load Time: The memory address is set when the program starts running. That is, the compiler generates relocatable code.<li>Execution Time: The OS determines where the addresses should be as it starts executing.</ul><h2 id=linking><a href=#linking aria-label="anchor link for linking">##</a> Linking</h2><p>Linking is the process of taking relocatable code and putting it in a single location and hooking up all libraries. There are two main ways to do this: <strong>static linking</strong> and <strong>dynamic linking</strong>.<p>Static linking is essentially where the linker essentially copies and pastes all the relevant code into the final binary. This is simpler, but does result in larger executable binaries. It also doesn't allow for updates without recompiling your project.<figure><img src=static_linking.png><figcaption><p>Static Linking</figcaption></figure><p>Dynamic linking is essentially where the linker sets up symlinks to all the relevant code into the final binary. This is more complicated but results in smaller executable binaries and allows for updates without recompiling. <em>Note:</em> This doesn't necessarily save space in memory once they're loaded unless you can share copies in memory (especially if you only use the base and limit registers are we are assuming). This can leave you in "DLL hell", where your code suddenly breaks because of silent/undeclared incompatibilities or where you can't tell what version of the library you're getting.<p>This can be more sophisticated using <strong>stubs</strong>. Stubs are small little pretend functions that actually properly link whenever they are first called.<figure><img src=dynamic_linking.png><figcaption><p>Dynamic Linking</figcaption></figure><h2 id=loading><a href=#loading aria-label="anchor link for loading">##</a> Loading</h2><p>Loading is where you actually load the library into memory (instead of just putting it into the binary). There are two ways to do this again: <strong>static loading</strong> and <strong>dynamic loading</strong>.<p>Static loading is the most common. When the OS loads your program, it loads your entire program into memory including your libraries.<p>Dynamic loading is like dynamic linking for loading. In dynamic loading, your stubs not only link to the proper library when they are run but they also actually load/locate the library in memory when it is executed. Likewise, you have dynamically unload libraries when they are used. This can reduce your program's memory footprint significantly, especially if you use some large library only in one place of your program or only occasionally in your program. It can additionally reduce start up time by not loading all of your libraries at first.<figure><img src=dynamic_loading.png><figcaption><p>Dynamic Loading</figcaption></figure><h2 id=swapping><a href=#swapping aria-label="anchor link for swapping">##</a> Swapping</h2><p>Swapping is the process of move your <em>main memory</em> into your <em>backing store</em> (normally disk). You may want to do this if your computer is running out of memory to run all of its processes.<p>When the OS detects that it will run out of memory, it will take a stopped process and move all of its memory to the backing store. Then, when that process wants to start, it swaps that memory back into main memory, potentially swapping something else to the backing store.<p>The biggest cost to swapping is <strong>transfer time</strong>, or the amount of time it takes to copy as process control block (PCB) to/from main memory.<p><em>Note:</em> Modern OSes don't always do this for one complete process at a time.<figure><img src=swapping.png><figcaption><p>Swapping</figcaption></figure><h2 id=making-room-for-memory><a href=#making-room-for-memory aria-label="anchor link for making-room-for-memory">##</a> Making Room for Memory</h2><p>Maintaining memory is difficult, we need to<ul><li>Maintain allocated regions,<li>Maintain unallocated regions,<li>Reclaim unused regions,<li>Efficiently allocate memory to minimize holes, and<li>Be quick.</ul><p><em>Note:</em> Maintaining memory is almost impossible for compile-time binding because those programs need to be loaded into an exact location. Load-time binding is much easier because they can fit anywhere as long as its contiguous.<p>There's a few main methods of figuring out what unused section / hole to allocate memory <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>n</mi></math></mjx-assistive-mml></mjx-container> words in. They each have strengths and weaknesses.<ul><li>First-Fit: Walk down the list and chose the first hole that's as large as n.<li>Best-Fit: Find the smallest hole that's as large as n.<li>Next-Fit: ADS<ul><li>Maintains good cache locality.</ul><li>Worst-Fit: Find the largest hole.<ul><li>This is good for storing holes in a heap.</ul></ul><p>As memory is allocated, we get small gaps in the memory between processes, too small to be useful. This is called <strong>external fragmentation</strong>. If you use first-fit memory allocation, on average about 50% of the memory you give out will be lost to external fragmentation.<p>Sometimes when processes ask for memory, you may give them more memory than they need. This is because it may be cheaper/simpler to not keep up with the little extra bit of unused memory. This is called <strong>internal fragmentation</strong>.<p><em>What if we have several allocated blocks that are not very tightly packed and we want to make them tightly packed?</em> You can perform <strong>compaction</strong> by moving them without the process noticing. You have to be very careful with this because all pointers and memory addresses will become invalidated. To resolve this, you need <em>execution time address binding</em>, often with hardware support.<p><em>Hardware support?</em> Yes! Instead of the process itself keeping actual addresses, they can keep addresses relative to some base/relocation register. Whenever the process uses an address, it is transparently mapped to an actual address by adding the value of the base/relocation register (or something more complex). Then the OS just needs to copy the memory and update the relocation register. This requires...<ul><li>Logical/Virtual Addresses: What the user program thinks it runs on.<li>Physical Addresses: What address the memory is <em>really</em> at, meaningful to hardware.<li>Address Translation: Runtime conversion from logical to physical address (using relocation register).</ul><p>Address translation is supported by the <strong>memory-management unit</strong> (MMU) and normally only applies in user-mode.<h2 id=paged-memory><a href=#paged-memory aria-label="anchor link for paged-memory">##</a> Paged Memory</h2><p>Our base and limit registers which we've been dealing with up until now have been fairly limited. Let's level them up using <strong>paged memory</strong>. This is a more sophisticated way of handling address translation. Paged memory isn't contiguous and is instead broken into multiple fixed-size blocks called <strong>pages</strong>. Process memory is split up into a sequence of pages physical memory is split into a sequence of page-sized <strong>frames</strong>.<p><em>Why differentiate between a page and a frame?</em> Frames are empty places where you "hang" pages.<figure><img src=paged_memory.png><figcaption><p>Paged Memory</figcaption></figure><p><em>Why do this?</em> This virtually eliminates external fragmentation by having hardware make fragmentation "normal".<p>Pages are organized into frames using a <strong>page table</strong>. The page table stores frames indexed by page. Whenever the CPU wants to read memory, it looks up the frame using the index of the page and translates uses that to translate the virtual/logical address into a physical address.<p>Each process has its own page table and it is used by the CPU in user mode.<p>Here is the algorithm for doing paged memory address translation. <em>Note:</em> For performance reasons, the page size is (almost) always a power of two. That way we just translate the high order bits using a table lookup and keep the low order bits, instead of doing arithmetic.<pre class="z-code language-python" data-lang=python><code class=language-python data-lang=python><span class="z-python z-source"><span class="z-meta z-python z-class"><span class="z-python z-class z-storage z-type"><span class="z-python z-keyword z-declaration z-class">class</span></span> <span class="z-python z-class z-entity z-name"><span class="z-meta z-python z-generic-name">Process</span></span><span class="z-punctuation z-section z-begin z-python z-class">:</span></span>
<span class="z-meta z-python z-function">    <span class="z-python z-function z-storage z-type"><span class="z-python z-function z-declaration z-keyword">def</span></span> <span class="z-python z-function z-entity z-name"><span class="z-python z-function z-magic z-support">__init__</span></span></span><span class="z-meta z-python z-function z-parameters"><span class="z-punctuation z-section z-begin z-python z-parameters">(</span></span><span class="z-meta z-python z-function z-parameters"><span class="z-python z-variable z-parameter">self</span><span class="z-punctuation z-python z-parameters z-separator">,</span> <span class="z-python z-variable z-parameter">page_size</span><span class="z-punctuation z-python z-parameters z-separator">,</span> <span class="z-python z-variable z-parameter">page_table</span><span class="z-punctuation z-section z-end z-python z-parameters">)</span></span><span class="z-meta z-python z-function"><span class="z-punctuation z-section z-begin z-python z-function">:</span></span>
        <span class="z-meta z-python z-qualified-name"><span class="z-python z-variable z-language">self</span><span class="z-punctuation z-python z-accessor z-dot">.</span><span class="z-meta z-python z-generic-name">page_size</span></span> <span class="z-python z-keyword z-operator z-assignment">=</span> <span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">page_size</span></span>
        <span class="z-meta z-python z-qualified-name"><span class="z-python z-variable z-language">self</span><span class="z-punctuation z-python z-accessor z-dot">.</span><span class="z-meta z-python z-generic-name">page_table</span></span> <span class="z-python z-keyword z-operator z-assignment">=</span> <span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">page_table</span></span>

<span class="z-meta z-python z-function">    <span class="z-python z-function z-storage z-type"><span class="z-python z-function z-declaration z-keyword">def</span></span> <span class="z-python z-function z-entity z-name"><span class="z-meta z-python z-generic-name">translate_address</span></span></span><span class="z-meta z-python z-function z-parameters"><span class="z-punctuation z-section z-begin z-python z-parameters">(</span></span><span class="z-meta z-python z-function z-parameters"><span class="z-python z-variable z-parameter">self</span><span class="z-punctuation z-python z-parameters z-separator">,</span> <span class="z-python z-variable z-parameter">addr</span><span class="z-punctuation z-section z-end z-python z-parameters">)</span></span><span class="z-meta z-python z-function"><span class="z-punctuation z-section z-begin z-python z-function">:</span></span>
        <span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">page_number</span></span> <span class="z-python z-keyword z-operator z-assignment">=</span> <span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">addr</span></span> <span class="z-python z-keyword z-operator z-arithmetic">/</span><span class="z-python z-keyword z-operator z-arithmetic">/</span> <span class="z-meta z-python z-qualified-name"><span class="z-python z-variable z-language">self</span><span class="z-punctuation z-python z-accessor z-dot">.</span><span class="z-meta z-python z-generic-name">page_size</span></span>
        <span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">frame</span></span> <span class="z-python z-keyword z-operator z-assignment">=</span> <span class="z-meta z-python z-item-access"><span class="z-meta z-python z-qualified-name"><span class="z-python z-variable z-language">self</span><span class="z-punctuation z-python z-accessor z-dot">.</span><span class="z-meta z-python z-generic-name">page_table</span></span></span><span class="z-meta z-python z-item-access"><span class="z-punctuation z-section z-begin z-brackets z-python">[</span></span><span class="z-meta z-python z-item-access z-arguments"><span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">page_number</span></span></span><span class="z-meta z-python z-item-access"><span class="z-punctuation z-section z-end z-brackets z-python">]</span></span>
        <span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">offset</span></span> <span class="z-python z-keyword z-operator z-assignment">=</span> <span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">addr</span></span> <span class="z-python z-keyword z-operator z-arithmetic">%</span> <span class="z-meta z-python z-qualified-name"><span class="z-python z-variable z-language">self</span><span class="z-punctuation z-python z-accessor z-dot">.</span><span class="z-meta z-python z-generic-name">page_size</span></span>
        <span class="z-python z-keyword z-control z-flow z-return">return</span> <span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">frame</span></span> <span class="z-python z-keyword z-operator z-arithmetic">*</span> <span class="z-meta z-python z-qualified-name"><span class="z-python z-variable z-language">self</span><span class="z-punctuation z-python z-accessor z-dot">.</span><span class="z-meta z-python z-generic-name">page_size</span></span> <span class="z-python z-keyword z-operator z-arithmetic">+</span> <span class="z-meta z-python z-qualified-name"><span class="z-meta z-python z-generic-name">offset</span></span>
</span></code></pre><figure><img src=logical_addresses.png><figcaption><p>Logical Addresses</figcaption></figure><h3 id=page-tables><a href=#page-tables aria-label="anchor link for page-tables">###</a> Page Tables</h3><p>How do we implement and store page tables? How big should they be? How do we tell the hardware where to look for the page table?<p>Currently, most page sizes are about 4 KiB. Modern hardware can support larger page tables, but this was common for 32 bit systems. That means, you'd have 12 bits for the addresses within a page and 20 bits for the page number. This means you'd have about 1 million entries in the page table, where each entry is at least 20 bits. Since 20 bits is a weird number, we normally give each entry 4 bytes (32 bits) because that makes it much easier to index into the table, so you don't have to multiply by a weird number (e.g. 3). This means the page table would take 4 MiB, with a lot of wasted space.<p><em>Note:</em> The above calculations assume you're using a 32 bit system. On a 64 bit system, you'd likely have 8 byte page entries and possibly more bits for a page number.<p><em>Note:</em> The size of the page numbers don't need to be the same size as the frame numbers. In other words, the <em>physical address and logical address don't need to be the same size</em>. This will allow you to either overcommit memory (what we do) or undercommit memory.<p>To help reduce the amount of wasted space, we use the extra space for other things. Here's a list of a few common uses:<ul><li>Valid/Invalid bit: If the bit is valid, the CPU allows you to access the page. Otherwise, you can't.<ul><li>Set by OS.<li>This is useful because it means the OS doesn't have to initialize all the memory you "have" and allows you to grow based off of what you use.</ul><li>Read-Only bit: True if this process can't write it.<ul><li>Set by OS.</ul><li>Modified bit: Whenever a page is written to.<ul><li>Set by hardware.<li>Useful for tracking whether or not we should copy out memory to disk if we're doing that.</ul><li>Reference bit: Set by hardware whenever page is read/written (referenced).<ul><li>Set by hardware.<li>This isn't set when you make an invalid read/write.</ul><li>No-Execute bit: True if process can't execute code on the page. (Your stack is normally not executable.)<ul><li>Set by hardware.<li>Used to help prevent buffer overflow, where people write arbitrary code into the stack and execute it.</ul></ul><p>We tell hardware what page we're using using the <strong>page table base register</strong> (PTBR). This can only be modified in kernel mode, as you'd expect. This is normally only switched during context switches by the OS.<h3 id=paged-memory-tricks><a href=#paged-memory-tricks aria-label="anchor link for paged-memory-tricks">###</a> Paged Memory Tricks</h3><p>Using paged memory allows us to do more clever things than just making memory allocation easier and reducing fragmentation. It allows us to do<ul><li>Sparsely Populated Logical Address Space: We can give processes a way larger logical address space than physical address space, basically preventing the stack and heap to bump into each other.<li>Shared memory: Just given processes pages with shared frames. Useful for mutable shared memory or shared libraries.<li>Copy on write: Only copy things once there are actual changes.<ul><li>Useful for efficient <code>fork</code> implementation! <code>fork</code> only copies the page table and sets the read-only bit on all pages. We set the read-only bit so the OS gets notified whenever the child tries to write to the page. Whenever they write, we actually copy the underlying frame and make both pages in both children writable.</ul></ul><h3 id=page-table-caching><a href=#page-table-caching aria-label="anchor link for page-table-caching">###</a> Page Table Caching</h3><p>Pages can be incredibly expensive. It doubles the number of memory accesses, since you always need to look at the page table, and requires the additional step of address translation.<p>This can be mitigated using a <strong>translation look-aside buffer</strong> (TLB), which is a special cache for each CPU core that caches a subset of the page table. The TLB stores the page table rows using associative memory that allows for parallel search by page number. In other words, it's a hardware dictionary / map.<p>If you can't find the row you're looking for in the TLB, it is called a <strong>TLB miss</strong> and we have to look up the frame number in the page table. We then save the just read row into the TLB (sometimes with a few of the surroundings), removing another row (decided by hardware). If you can find what you want, it's called a <strong>TLB hit</strong> and you don't have to look at the page table.<figure><img src=tlb.png><figcaption><p>Paging with TLB</figcaption></figure><p><em>Note:</em> In class we normally treat the TLB as a write-back cache. It's also normally a write-back cache.<p>To measure the performance of the TLB, we have to consider the <strong>TLB Hit Ratio</strong> (<mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D6FC"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>α</mi></math></mjx-assistive-mml></mjx-container>). Then, the <strong>effective access time</strong> is <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c210E"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D456"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D6FC"></mjx-c></mjx-mi><mjx-mo class=mjx-n space=3><mjx-c class=mjx-c2B></mjx-c></mjx-mo><mjx-msub space=3><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45A"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D456"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D460"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D460"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=mjx-n><mjx-c class=mjx-c28></mjx-c></mjx-mo><mjx-mn class=mjx-n><mjx-c class=mjx-c31></mjx-c></mjx-mn><mjx-mo class=mjx-n space=3><mjx-c class=mjx-c2212></mjx-c></mjx-mo><mjx-mi class=mjx-i space=3><mjx-c class="TEX-I mjx-c1D6FC"></mjx-c></mjx-mi><mjx-mo class=mjx-n><mjx-c class=mjx-c29></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>h</mi><mi>i</mi><mi>t</mi></mrow></msub><mi>α</mi><mo>+</mo><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>m</mi><mi>i</mi><mi>s</mi><mi>s</mi></mrow></msub><mo stretchy=false>(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy=false>)</mo></math></mjx-assistive-mml></mjx-container>. We can calculate <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c210E"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D456"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>h</mi><mi>i</mi><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45A"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D456"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D460"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D460"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>m</mi><mi>i</mi><mi>s</mi><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> using the TLB lookup time (<mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D459"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D44F"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>t</mi><mi>l</mi><mi>b</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>) and memory cycle time (<mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45A"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D452"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45A"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45C"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45F"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D466"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>m</mi><mi>e</mi><mi>m</mi><mi>o</mi><mi>r</mi><mi>y</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>) doing <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c210E"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D456"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=mjx-n space=4><mjx-c class=mjx-c3D></mjx-c></mjx-mo><mjx-msub space=4><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D459"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D44F"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=mjx-n space=3><mjx-c class=mjx-c2B></mjx-c></mjx-mo><mjx-msub space=3><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45A"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D452"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45A"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45C"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45F"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D466"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>h</mi><mi>i</mi><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>t</mi><mi>l</mi><mi>b</mi></mrow></msub><mo>+</mo><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>m</mi><mi>e</mi><mi>m</mi><mi>o</mi><mi>r</mi><mi>y</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45A"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D456"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D460"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D460"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=mjx-n space=4><mjx-c class=mjx-c3D></mjx-c></mjx-mo><mjx-msub space=4><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D459"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D44F"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=mjx-n space=3><mjx-c class=mjx-c2B></mjx-c></mjx-mo><mjx-mn class=mjx-n space=3><mjx-c class=mjx-c32></mjx-c></mjx-mn><mjx-msub><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D461"></mjx-c></mjx-mi><mjx-script style=vertical-align:-.15em><mjx-texatom texclass=ORD size=s><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45A"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D452"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45A"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45C"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45F"></mjx-c></mjx-mi><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D466"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>m</mi><mi>i</mi><mi>s</mi><mi>s</mi></mrow></msub><mo>=</mo><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>t</mi><mi>l</mi><mi>b</mi></mrow></msub><mo>+</mo><mn>2</mn><msub><mi>t</mi><mrow data-mjx-texclass=ORD><mi>m</mi><mi>e</mi><mi>m</mi><mi>o</mi><mi>r</mi><mi>y</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>.<p>We want to maximize the TLB hit ratio so we get good performance. How do we do that? Using <strong>locality of reference</strong> or, in other words, putting everything close together so they fall mostly in the same pages.<p>Since the page table is different between processes, most OS's throw away the TLB every time there is a context switch, which means that after every context switch you start anew with the cache. This is one reason why context switches are so expensive and in-process context switches are cheap.<p>Alternatively, some hardware marks every TLB entry with a <em>address-space identifier</em> unique to the process and only allows the process with the correct identifier to access values of the TLB. This let's you preserve the TLB between switches.<h3 id=making-the-page-table-manageable><a href=#making-the-page-table-manageable aria-label="anchor link for making-the-page-table-manageable">###</a> Making the Page Table Manageable</h3><p>As we discussed earlier, if we naively implemented page tables with <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-msup><mjx-mn class=mjx-n><mjx-c class=mjx-c32></mjx-c></mjx-mn><mjx-script style=vertical-align:.363em><mjx-texatom texclass=ORD size=s><mjx-mn class=mjx-n><mjx-c class=mjx-c32></mjx-c><mjx-c class=mjx-c30></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><msup><mn>2</mn><mrow data-mjx-texclass=ORD><mn>20</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> entries at 4-bytes each, every process would have a 4MiB page table, which doesn't scale well, especially considering very few processes actually use it all.<h4 id=variable-page-table-size><a href=#variable-page-table-size aria-label="anchor link for variable-page-table-size">####</a> Variable Page Table Size</h4><p><em>Simplest</em>.<p>Variable page table size just allows processes to have a smaller page table and only access certain possible pages. This is normally implemented using a <strong>page table length register</strong> (PTLR) that tells the hardware the length of the current page table and then the hardware checks the length on every access. This is saved and restored on context switch.<p>This makes growing page tables very problematic because you have to both find more memory for the frames and the larger page table.<h4 id=hierarchical-page-tables><a href=#hierarchical-page-tables aria-label="anchor link for hierarchical-page-tables">####</a> Hierarchical Page Tables</h4><p><em>By far most common</em>.<p>This resolves the problem of finding contiguous memory regions for page tables the same way page tables do. <em>A meta-page table!</em> (Or a hierarchical page table.)<p>To implement this, we break up the page table into page-sized section. This means we can mark large sections of the page table as invalid and not allocate memory for them by invalidating a page in the outer page table. Then, we can easily grow the page table when we need to by allocating a page in memory for the page table (which then gets pages to fill itself in).<p><em>Note:</em> Technically at full load, this takes up more memory than a naive implementation, but that almost never happens.<p><em>Note:</em> Really, both the inner and outer page table are in the same frames that normal memory is placed in.<p>To do address translation, we split up our logical address into three parts (unlike the normal 2). The highest order bits are for the outer page table, the middle order bits are for the inner page table, and the low order bits for the final page offset.<p>We can actually have more than the 2 levels discussed here! The technique works for arbitrarily many until the top level table fits in a page.<p>With multi-level page tables, the TLB becomes increasingly important. This is because the TLB goes straight from page number (including outer and lower page number) to the frame number. Thus you can skip each of the levels and go straight to the frame number, saving two memory accesses (one for outer and one for inner page table).<p><em>Note:</em> All page tables except for the outermost one are guaranteed to have the same page size because they fit as much as they can in a frame. You'd like it to work out perfectly so that the outermost page table has no wasted space.<h4 id=hashed-page-tables><a href=#hashed-page-tables aria-label="anchor link for hashed-page-tables">####</a> Hashed Page Tables</h4><p>Store the sparse mapping from page number to frame number as a hash table. To handle collisions, you use a linked list on every line of the hash table. Then the hardware just hashes the page number looks in the list for a match.<h4 id=inverted-page-table><a href=#inverted-page-table aria-label="anchor link for inverted-page-table">####</a> Inverted Page Table</h4><p>Since we only have a limited number of frames that doesn't change between processes, if we build the table backwards, it can be much smaller. This has more complicated page lookup, but can have decreased storage overhead.<p>The reason this saves so much memory is because there are often fewer physical frames than logical pages and you only have a single shared page table for the entire machine. Because the page table has to be shared, you need to verify the PID of the process.<p>This would have a huge performance disadvantage if it weren't for the TLB.<h4 id=software-address-translation><a href=#software-address-translation aria-label="anchor link for software-address-translation">####</a> Software Address Translation</h4><p>For more complex address translation methods (e.g. hashed page tables), sometimes the hardware doesn't support address translation through anything but the TLB. If it tries to use a page number not in the TLB, it traps the OS and asks it to do the address translation and put the output in the TLB. This makes TLB misses way more complicated, but means you can make the page table more sophisticated.<h2 id=virtual-memory><a href=#virtual-memory aria-label="anchor link for virtual-memory">##</a> Virtual Memory</h2><p>Previously we talked about swapping entire process's memory to secondary storage. This was extremely disruptive to the process, not very granular, and not particularly efficient, because we had to do so much at one time.<p>We can make this swapping more granular by only swapping a single page at a time. Here's some terms about that.<ul><li>Page out: Copy a page from main memory to the backing store.<li>Page in: Copy a page from the backing store into main memory.<li>Memory resident: The page is in a frame in main memory.</ul><p>What are some strategies to do this?<h3 id=demand-paging><a href=#demand-paging aria-label="anchor link for demand-paging">###</a> Demand Paging</h3><p>We move seldom-used pages to the backing store and bring a page into memory when processes try to access it. This means processes take much less physical memory.<p>This can be <strong>pure</strong> demand paging, where we don't even bother loading pages into memory until they're first used. This can save I/O at process startup and allows less physical memory per process, allowing more processes.<p><em>How do we know when we should page in a page when it isn't in memory?</em> The trick is we mark the pages that aren't memory resident (on secondary storage) as invalid. Then when the process tries to access one of these pages, the hardware traps to the OS and we can figure out if the page actually exists but just in secondary storage.<p>While we are paging in the page a process asked for, we block the process on I/O (since paging in can take awhile!). When the OS is done paging in the page, the OS updates the process's page table and wakes up the process.<h3 id=page-fault-efficiency><a href=#page-fault-efficiency aria-label="anchor link for page-fault-efficiency">###</a> Page Fault Efficiency</h3><p><em>What if paging in requires we page out another page?</em> In that case, we call it <strong>page replacement with a victim</strong>, where the victim page is the page that is being forcibly paged out. This means page faults / swapping pages can be <strong>really</strong> expensive because you have to read-in, write-out, and do other administrative work to do page replacement.<p>With this knowledge, we can create a formula for our <strong>effective access time</strong> (EAT), that is how long it takes to access memory on average, where <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mn class=mjx-n><mjx-c class=mjx-c30></mjx-c></mjx-mn><mjx-mo class=mjx-n space=4><mjx-c class=mjx-c2264></mjx-c></mjx-mo><mjx-mi class=mjx-i space=4><mjx-c class="TEX-I mjx-c1D45D"></mjx-c></mjx-mi><mjx-mo class=mjx-n space=4><mjx-c class=mjx-c2264></mjx-c></mjx-mo><mjx-mn class=mjx-n space=4><mjx-c class=mjx-c31></mjx-c><mjx-c class=mjx-c2E></mjx-c><mjx-c class=mjx-c30></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mn>0</mn><mo>≤</mo><mi>p</mi><mo>≤</mo><mn>1.0</mn></math></mjx-assistive-mml></mjx-container> is the page fault rate. <code>EAT = (1-p)*memory_access_time + p*(page_fault_overhead + page_out_time + page_in_time + fix_page_table + memory_access_time)</code>.<p>Normally, the amount of time it takes to handle a page fault is at least 5-6 orders of magnitude above the non-page fault memory access time. This means we <em><strong>really</strong></em> don't want to page fault.<p>We can't pick what memory the process's ask for, so the only thing that we can really control to keep our page fault rate low is who should be the victim. <em>Note:</em> We don't have long to think about this because you're the OS!<p>There are two main policies for picking the victim. <strong>Local</strong> page replacement policy means that a process will only ever replace one of its own pages. <strong>Global</strong> page replacement policy means that any page can be the victim when a process needs to replace a page.<h3 id=page-replacement-algorithms><a href=#page-replacement-algorithms aria-label="anchor link for page-replacement-algorithms">###</a> Page Replacement Algorithms</h3><p>We test this similar to how we tested process scheduling algorithms. We'll make up a string of pages used by a "program" and see how various algorithms perform. The most thing we measure is the number of page faults.<p>Belady's anomaly occurs when more memory results in worse performance. It can occur at any level, not just swapping pages. Not all algorithms are subject to Belady's anomaly, but many are.<p>Algorithms that aren't subject to Belady's anomaly are called <strong>stack algorithms</strong>. Stack algorithms are algorithms where the set of pages you keep in memory is a subset of the set of pages you would have kept in memory if you had one more page. In other words, more memory always means you'll keep additional pages in memory but never chose to throw out memory that would have remained with fewer pages.<h4 id=first-in-first-out-fifo-page-replacement><a href=#first-in-first-out-fifo-page-replacement aria-label="anchor link for first-in-first-out-fifo-page-replacement">####</a> First-In-First-Out (FIFO) Page Replacement</h4><p>Always pick the oldest page as the victim.<ul><li>Pros:<ul><li>Extremely easy and efficient to implement<li>Only requires work when a page fault occurs. (Normal memory access doesn't require updating anything.)</ul><li>Cons:<ul><li>Often we have long running processes that are important.<li>Really sucks when we access memory in repeated sequences.<li>More frames can yield more page faults. <strong>Belady's anomaly</strong>.</ul></ul><h4 id=optimal-page-replacement-opt><a href=#optimal-page-replacement-opt aria-label="anchor link for optimal-page-replacement-opt">####</a> Optimal Page Replacement (OPT)</h4><p>This algorithm is guaranteed to give you the fewest number of page faults for a given reference string.<p>OPT looks into the future by looking at the references after its current position in the reference string. It then picks the reference that won't be used for the longest time as the victim.<ul><li>Pros:<ul><li>Guaranteed to give you the fewest number of page faults.</ul><li>Cons:<ul><li>Requires you know the order of page requests that will occur in the future to be properly implemented.</ul></ul><p>To get around the knowing the future, we use heuristics based of previous behavior to predict the future.<h4 id=least-recently-used-lru><a href=#least-recently-used-lru aria-label="anchor link for least-recently-used-lru">####</a> Least Recently Used (LRU)</h4><p>This is like a backward-looking approximation looking of OPT. The idea is that pages that have been used recently are likely to be used in the future.<p>To implement this, you'd probably either keep a timestamp on every memory reference and then replace the page with the oldest timestamp (okay in hot path, bad in cold path.) or you could keep a linked list and move the page to the front of the list (awful in hot path, okay in cold path.) Since this is hard to implement efficiently, we normally approximate it.<p>One way to approximate this is to use the hardware-set <strong>reference bit</strong>. The OS clears the bit every once in awhile to see get an idea on pages are being used most often. This leads us into the <strong>Second Chance algorithm</strong>.<h4 id=second-chance-clock-algorithm><a href=#second-chance-clock-algorithm aria-label="anchor link for second-chance-clock-algorithm">####</a> Second Chance / Clock Algorithm</h4><p>We keep a pointer to the next victim page. When we go to page-out / evict the victim, we check its reference bit. (We do this even when evicting uninitialized pages.) If its reference bit is set, we clear the bit and move to the next page. If the reference bit is set, it is your victim.<p>This is good because it is free in the hot path and cheap in the cold path.<p>This algorithm performs better under pressure because it is a better approximation to LRU the faster it cycles around. So, if it doesn't cycle around for a really long time, it basically just randomly picks a page to evict after moving the victim pointer around a lot and clearing a bunch of reference bits.<h4 id=least-frequently-used><a href=#least-frequently-used aria-label="anchor link for least-frequently-used">####</a> Least Frequently Used</h4><p>This utilizes the reference bit like the second change algorithm does. Every once in awhile the OS goes in and checks the reference bits. If the bit is set, it increments the pages corresponding count and then clears the bit.<p>Then, when it needs to evict a page, it evicts the page with the smallest count.<p>This solution generally has a problem with over-valuing pages that are used intensively for a short period of time and then not used for a long period of time. We could however find a way to decrement the counter properly.<p><em>Note:</em> Sometimes LRU is absolutely awful, even in fairly common places. Imagine you access pages in the same order, over and over, (e.g. iterating over a large array) and you have almost enough frames to hold them all. This would cause you to page fault over and over and over again. Because of this, some systems implement multiple page replacement algorithms, each with a priority. When a process starts it uses the highest priority one, but if that keeps failing then it switches to the next one, eventually even settling for random page replacement.<h4 id=additional-reference-bits><a href=#additional-reference-bits aria-label="anchor link for additional-reference-bits">####</a> Additional Reference Bits</h4><p>This is similar to least frequently used, but it keeps a history of the past few reference bits. To do this, every page has a reference register or reference integer. Periodically the OS goes in and right shifts all reference registers and bitwise ors the current reference bit with the most significant bit in the register. Then, when it's time to pick a register, it picks the one with the smallest count.<p>Here's an example<pre class="z-code language-nohighlight" data-lang=nohighlight><code class=language-nohighlight data-lang=nohighlight><span class="z-plain z-text">0011 1111  # used for awhile but not recently
1100 0000  # idle for awhile but used recently
1100 0110  # used recently and in past
1111 1111  # used all the time
</span></code></pre><p>This is clever and not incredibly expensive, but it's still fairly expensive because it has to do the upkeep of occasionally checking. Since page faults don't happen that often, the additional upkeep and harder choosing process of this better algorithm normally isn't worthwhile.<h3 id=enhanced-second-chance><a href=#enhanced-second-chance aria-label="anchor link for enhanced-second-chance">###</a> Enhanced Second Chance</h3><p>This is just second chance except we prefer pages that have not been written to. We want to prefer this because it means paging out is cheaper, since we don't need to write the updated page into its copy in storage. To do this, we look at both the reference bit and the <strong>dirty bit</strong>.<p>The following table shows the priority of the enhanced second chance algorithm. It prefers the pages at the top. The reason we prefer dirty pages that haven't been recently used is because we consider the reference bit to say whether we'll need to page something back in in the future. If we page out a clean recently used page, we'll have to page it back in later at a cost of 1. If we page out a dirty not recently used page, it'll have a cost of 1 as well because we'll only need to page it out.<table><thead><tr><th>Reference<th>Dirty<tbody><tr><td><code>0</code><td><code>0</code><tr><td><code>0</code><td><code>1</code><tr><td><code>1</code><td><code>0</code><tr><td><code>1</code><td><code>1</code></table><p><em>How does it make its choice?</em> As it scans forward, it acts like normal second chance but it will only accept clean pages. However, while it is scanning it remembers the state of other pages. If it scans everything and doesn't find a clean, unused page, it goes through its memory to settle for something else.<h3 id=page-fault-tricks><a href=#page-fault-tricks aria-label="anchor link for page-fault-tricks">###</a> Page Fault Tricks</h3><p>You can do some things to reduce the cost of page faults.<p>One method is <strong>page buffering</strong>. To do this, you keep some frames free all the time. When a page fault occurs, you page your desired page into one of the available frames. Then, you later page out an idle page to maintain your buffer / pool of free frames. <em>This is like paying in advance.</em> You basically hope that some pages won't be missed and your disk will sometimes be free.<p>Page buffering also allows for <strong>minor page faults</strong>. That occurs when you need a page that hasn't been used since it was paged out onto disk. Since we didn't actually overwrite the contents of the page, we don't need to do any I/O and can just resurrect that old page. (We call "normal" page faults <strong>major page</strong> faults in this case!) This is kinda like the OS being caught thinking that a page wasn't used anymore.<p>Minor page faults also allow for smarted shared libraries. When the process first tries to access the shared library that it hasn't loaded into its page table, the OS probably can handle it using a minor page fault if any other process has the bit of the shared library loaded that we want.<h3 id=thrashing><a href=#thrashing aria-label="anchor link for thrashing">###</a> Thrashing</h3><p>Thrashing occurs when a process doesn't have enough pages to do the work it requires. It starts to page fault <em>a lot</em> and exhibit low CPU utilization and abnormally high I/O utilization, leading to an extreme decrease in overall performance.<p><em>How do we tell the difference between many I/O bound processes and thrashing?</em> Normally, you monitor the page fault rate. If page faults are where the majority of I/O is occurring, then you're most likely thrashing.<p><em>How do we mitigate thrashing?</em> One way to mitigate thrashing is to implement local page replacement. That way one process's bad behavior (thrashing) doesn't spread to the entire rest of the system.<h3 id=local-page-replacement><a href=#local-page-replacement aria-label="anchor link for local-page-replacement">###</a> Local Page Replacement</h3><p>The most common way to allocate frames for an individual process is to do <strong>proportional allocation</strong>, where you give each process a number of frames proportional to its total memory size. However, this has an issue. Process's need a minimum number of frames or else they have the potential to stop making process, since one instruction can potentially require many frames at once if, for example, the instruction spans a frame, the source and destination are in separate frames, etc. Therefore, OSes that do proportional allocation have a <em>minimum</em> number of frames that they allocate to a process. They want to allocate more than this minimum in general though for good performance.<p><em>How do we quantify a processes page usage?</em> We consider the process's <strong>working set</strong> or the pages it is actively using right now. If we want to measure the process's working set, we can count the most recent <mjx-container class=MathJax jax=CHTML style=font-size:113.1%;position:relative><mjx-math aria-hidden=true class=MJX-TEX><mjx-mi class=mjx-i><mjx-c class="TEX-I mjx-c1D45B"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml display=inline unselectable=on><math xmlns=http://www.w3.org/1998/Math/MathML><mi>n</mi></math></mjx-assistive-mml></mjx-container> references (say 1,000,000). If we can't keep the process's working set in memory, we'll have very poor performance. In general, processes that have a smaller working set will have better performance because they are less likely to page fault; processes that exhibit better <em>locality of reference</em> generally have better performance.<p>This working set can change though as the program executes, so the operating system should have a way to adjust its frame allocation policy for a process as it executes. We normally do this through heuristics, rather than actually keeping track of the process's working set. A standard heuristic to keep is the <em>page fault rate</em> of the process.<h3 id=page-replacement-in-linux-2-4><a href=#page-replacement-in-linux-2-4 aria-label="anchor link for page-replacement-in-linux-2-4">###</a> Page Replacement in Linux 2.4</h3><p>The kernel maintains the following lists of pages.<ul><li>Active list: Pages that aren't (currently) candidates for replacement.<li>Inactive lists: Pages that would (probably) make good victims.<ul><li>Clean list: Pages that are identical to a copy on the backing store.<li>Dirty list: Pages that are not (or at least might not be).</ul></ul><p>Every once in awhile, the kernel processes the active list like second change. If the page was referenced since it last checked, it increments the age. If it wasn't referenced and the age isn't 0, it halves the age. Otherwise, it moves the page to the appropriate inactive list.<p>Occasionally the kernel will clean pages on the dirty list (i.e. write them out to the backing store and move them to the clean list). Pages on the clean list are potential minor page faults and as such are checked before page faulting.<h1 id=gpu-programming-cuda><a href=#gpu-programming-cuda aria-label="anchor link for gpu-programming-cuda">#</a> GPU Programming & CUDA</h1><p>CUDA is a framework for general purpose GPU programming. CUDA is nice because it helps factor our hardware differences across GPU models and mix CPU and GPU code.<p>CPUs have a few powerful processing elements that are independently controllable with large caches. GPUs meanwhile have <em>many</em> weak processing elements that are controlled in concert and each doing pretty much the same thing with small caches. That is, GPUs follow the <strong>SIMD</strong> or single instruction multiple data design.<p><em>Note:</em> The cache of GPUs has a different idea of locality, because you want to consider pixels vertically and horizontally adjacent to be "local".<h2 id=jobs-on-the-gpu><a href=#jobs-on-the-gpu aria-label="anchor link for jobs-on-the-gpu">##</a> Jobs on the GPU</h2><p>In GPU programming, the CPU (or <strong>host</strong>) is in charge while the GPU (or <strong>device</strong>) does most of the work. The host executes the serial parts of the program, prepares jobs for the device, and requests and waits for jobs/grids to execute. The device actually does the work.<p>How do you run jobs on the GPU? You create a <strong>block</strong> of threads, each contains 10-100s threads, with shared memory and limited synchronization support (e.g. wait for everyone else to catch up). <em>You can still have race conditions with shared memory and such like you would normally.</em><p>Because thread blocks have shared memory and synchronization support, there's a limit on how big they can be. To get around this, we organize blocks of threads into <strong>grids</strong> or <strong>jobs</strong>, where grids can be multi-dimensional. Each block has an index within its grid and each thread has an index within its block. It uses these indexes to determine what computations it's responsible for.<p><em>Why are they organized into grids?</em> It allows us to better model the idea of graphics and images and also take advantage of GPUs different cache locality model. We won't take advantage of that here though.<p><em>Note:</em> You want to break your kernel up to be as small as possible, even if you think doing multiple might be better. GPU processing elements are incredibly weak but they are incredibly parallel.<h2 id=cuda-code><a href=#cuda-code aria-label="anchor link for cuda-code">##</a> CUDA Code</h2><p>CUDA uses an extended version of the C language (<code>.cu</code>), with additional attributes to mark where the function will run, that way it can properly compile the code.<pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-c z-source"><span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> b needs to be in host memory
</span>__host__ <span class="z-c z-storage z-type">void</span> <span class="z-c z-meta z-function"><span class="z-c z-function z-entity z-name">someFunction</span></span><span class="z-c z-meta z-function z-parameters"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function z-parameters"><span class="z-c z-meta z-group"><span class="z-c z-storage z-type">int</span> <span class="z-c z-variable z-parameter">a</span><span class="z-c z-punctuation z-separator">,</span> <span class="z-c z-storage z-type">char</span> <span class="z-c z-keyword z-operator">*</span><span class="z-c z-variable z-parameter">b</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-meta z-function"> </span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span></span></span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block">
  <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> C code to run on the host (or CPU).
</span></span></span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-end z-block">}</span></span></span>

<span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> d needs to be in device memory
</span>__device__ <span class="z-c z-storage z-type">void</span> <span class="z-c z-meta z-function"><span class="z-c z-function z-entity z-name">otherFunction</span></span><span class="z-c z-meta z-function z-parameters"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function z-parameters"><span class="z-c z-meta z-group"><span class="z-c z-storage z-type">int</span> <span class="z-c z-variable z-parameter">c</span><span class="z-c z-punctuation z-separator">,</span> <span class="z-c z-storage z-type">double</span> <span class="z-c z-keyword z-operator">*</span><span class="z-c z-variable z-parameter">d</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-meta z-function">
</span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span></span></span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block">
  <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> C code to run on the device (or GPU).
</span></span></span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-end z-block">}</span></span></span>
</span></code></pre><p>Each thread in a grid executes a <strong>kernel</strong> (no relation to OS kernel). A kernel is a function that runs on the device but can be called from the host.<pre class="language-c z-code" data-lang=c><code class=language-c data-lang=c><span class="z-c z-source"><span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> Define a kernel that will execute on the
</span><span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> device and be called from the host.
</span>__global__ <span class="z-c z-storage z-type">void</span> <span class="z-c z-meta z-function"><span class="z-c z-function z-entity z-name">myKernel</span></span><span class="z-c z-meta z-function z-parameters"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function z-parameters"><span class="z-c z-meta z-group"><span class="z-c z-storage z-type">int</span> <span class="z-c z-variable z-parameter">a</span><span class="z-c z-punctuation z-separator">,</span> <span class="z-c z-storage z-type">int</span> <span class="z-c z-keyword z-operator">*</span><span class="z-c z-variable z-parameter">bList</span><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-meta z-function"> </span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span></span></span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block">
  <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> C code for the device to execute.
</span></span></span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-end z-block">}</span></span></span>

<span class="z-c z-storage z-type">int</span> <span class="z-c z-meta z-function"><span class="z-c z-function z-entity z-name">main</span></span><span class="z-c z-meta z-function z-parameters"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span></span></span><span class="z-c z-meta z-function z-parameters"><span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-end z-group">)</span></span></span><span class="z-c z-meta z-function"> </span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-begin z-block">{</span></span></span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block">
  <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> Call a kernel from the host, asking the
</span>  <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> device to execute. NOTE: bList must be in
</span>  <span class="z-c z-comment z-double-slash z-line"><span class="z-c z-punctuation z-comment z-definition">//</span> device memory
</span>  myKernel<span class="z-c z-keyword z-operator z-arithmetic">&lt;&lt;</span><span class="z-c z-keyword z-operator z-comparison">&lt;</span>blocksPerGrid<span class="z-c z-punctuation z-separator">,</span> threadsPerBlock<span class="z-c z-keyword z-operator z-arithmetic">>></span><span class="z-c z-keyword z-operator z-comparison">></span>
  <span class="z-c z-meta z-group"><span class="z-c z-punctuation z-section z-begin z-group">(</span>a<span class="z-c z-punctuation z-separator">,</span> bList<span class="z-c z-punctuation z-section z-end z-group">)</span></span><span class="z-c z-punctuation z-terminator">;</span>
</span></span><span class="z-c z-meta z-function"><span class="z-c z-meta z-block"><span class="z-c z-punctuation z-section z-end z-block">}</span></span></span>
</span></code></pre><p>To compile CUDA code, you use <code>nvcc</code>. <code>nvcc</code> presents a similar CLI to <code>gcc</code> and <code>clang</code>. Once you compile the CUDA code, you execute it as you normally would.<h2 id=memory-2><a href=#memory-2 aria-label="anchor link for memory-2">##</a> Memory</h2><p>The GPU has memory which the CPU cannot address. Therefore, we cannot use our normal memory techniques (e.g. <code>malloc</code>). Luckily, CUDA provides calls that are similar to standard memory allocation calls.<ul><li><code>cudaError_t cudaMalloc(void **memp, size_t size)</code>: Fill in <code>memp</code> with a pointer to the newly allocated block of <code>size</code> bytes.<ul><li>Don't dereference the memory in the host!</ul><li><code>cudaError_t cudaFree(void *mem)</code>: Free the memory returned by <code>cudaMalloc</code>.<li><code>cudaError_t cudaMemcpy(void *dest, const void *src, size_t size, enum cudaMemcpyKind dir)</code>: Copy memory between the CPU and GPU (either direction, specified by <code>dir</code>).<li><code>cudaError_t cudaDeviceReset()</code>: Remove all device allocations.</ul><h2 id=things-we-missed><a href=#things-we-missed aria-label="anchor link for things-we-missed">##</a> Things We Missed</h2><p>CUDA has per-thread registers, per-thread local memory, per-grid global memory, and per-grid constant memory. It also has the <code>__syncthreads()</code> barrier within a block and atomic operations on shared memory.<h1 id=distributed-systems><a href=#distributed-systems aria-label="anchor link for distributed-systems">#</a> Distributed Systems</h1><p>Benefits of distributed computing:<ul><li>Resource sharing: Only need one machine with a certain resource.<li>Computational speedup: More hosts mean more power.<li>Reliability: Individual nodes can fail, but the system can still operate.<li>Communication: Mostly for humans.<li>Cost effectiveness: Multiple inexpensive devices are cheaper and easier to grow.</ul><p>Building distributed operating systems are a lot like operating systems. It could transparently provide resources within the network and perform other tasks such as:<ul><li>Data migration: Transparently move data as necessary.<li>Computation migration: Execute computational steps on host with needed resources.<li>Process migration: Move a whole program while it is running. (Load balancing, data access, etc.)</ul><p>In general, distributed systems need the following properties:<ul><li>Scalability: Gradually add resources.<li>Robustness: Tolerate failure of individual components.<li>Availability: High utilization.<li>Heterogeneity: Different types of hosts can cooperate. Differences can include different computational speeds, byte order, instruction sets, operating system, etc.</ul><p>There have been a lot of work on distributed computing. Although there is still much work to be done, we have made some progress.<ul><li>Operating systems provide easy access to the network.<li>Some distributed file systems do exist.<li>Map-reduce for distributed high performance computing has been successful.</ul><h2 id=shareable-network-interface><a href=#shareable-network-interface aria-label="anchor link for shareable-network-interface">##</a> Shareable Network Interface</h2><p>Much like we built a file system to both make using the storage device easier and making it so that multiple processes can use the storage device simultaneously, we build a network interface.<p>Here's a list of issues this network interface has to handle:<ul><li>Indirect Connections: We aren't always connected directly to the host we want.<li>Unreliable Connections: Messages may be lost or delayed and links may fail.<li>Network Congestion: We're sharing the network with other machines on the network, so we have to be able to route messages even with antagonists / competing applications.<li>Changing Network Topology: Devices come online, offline, and switch networks all the time.</ul><p>Normally, to handle all these concerns we split the concerns into layers, where each layer handles a particular set of concerns.<h3 id=physical-layer><a href=#physical-layer aria-label="anchor link for physical-layer">###</a> Physical Layer</h3><p>Decides how we will physically connect machines and how we will encode information (i.e. binary).<p>Part of Ethernet and part 802.11 (wireless) handles this.<h3 id=data-link-layer><a href=#data-link-layer aria-label="anchor link for data-link-layer">###</a> Data Link Layer</h3><p>Organizes data into fixed or variable length frames and makes routing decisions for local networks. Corrects errors introduced by physical layer (normally using information stored in message header).<p>The rest of Ethernet and 802.11 (wireless) handles this. Also modem.<h3 id=network-layer><a href=#network-layer aria-label="anchor link for network-layer">###</a> Network Layer</h3><p>Organize data (frames) into variable-length packets. Adds additional header information concerning routing data across multiple connections.<p>The Internet Protocol (IP) handles this.<p>The Internet Protocol provides a best-effort packet delivery service. Each host is represented with a 32-bit IPv4 address (or 128-bit for IPv6). Each packet is stamped with a source and destination address. Basically, you dump a packet into the network and it tries its best to route it to the destination.<p>There is no error detection or correction for the payload (only for the headers). There is no attempt to avoid network congestion.<p>Congestion may cause packets to be dropped (or more rarely errors) or even duplicated. Retransmission to recover from lost packets may reorder packets (or more rarely routing machines).<p>Individual IP addresses have a structure that allows routers to know who would know more about the packet. They forward the packet to this node and it continues until it reaches the destination.<h3 id=transport-layer><a href=#transport-layer aria-label="anchor link for transport-layer">###</a> Transport Layer</h3><p>The transport layer provides useful application semantics on top of the network layer. It allows for process-to-process packet delivery (rather than host-to-host). Breaks (large) messages into packets and reassembles them on arrival. Creates the illusion of connections for clients, rather than just spurious packets/messages.<p>Examples include the Transmission Control Protocol (TCP) or User Datagram Protocol (UDP).<h4 id=udp><a href=#udp aria-label="anchor link for udp">####</a> UDP</h4><p>UDP is conceptually a very thin layer. You don't need to establish a connection first and messages may be lost. Conceptually, it is just a message queue that provides error detection within messages and port numbers to dispatch messages to the right process on the host.<p><em>Note:</em> Individual messages are called <strong>datagrams</strong>.<p>Port numbers are 16-bit integers that uniquely identify processes on a host.<p>To use UDP, you choose a port number and then create a datagram socket for that port.<p><em>Why is UDP set up?</em> UDP is very cheap to set up and because of that fast. For certain services where real-time performance is more important than never missing a message (e.g. streaming video), this trade off is worth it.<h5 id=udp-in-java><a href=#udp-in-java aria-label="anchor link for udp-in-java">#####</a> UDP in Java</h5><p>You just use <code>java.net.DatagramSocket</code> and <code>java.net.DatagramPacket</code>. You create a <code>DatagramSocket</code> on a particular client. You create <code>DatagramPacket</code>s, which have both data and a destination (i.e. IP and port), and have the socket send these packets to the given destination.<h4 id=tcp><a href=#tcp aria-label="anchor link for tcp">####</a> TCP</h4><p>Whereas UDP was a message queue, TCP is a <em>pipe</em>. TCP has no message boundaries (instead just sending bytes) and guarantees that no bytes will be lost or received out of order. A connection must be established first and reliability (e.g. packet loss prevention) is achieved by using acknowledgement and a timeout, retransmitting in the case of failure. It also performs <em>congestion control</em>, where it will slow down the sender if the receivers network is congested / we're losing too many packets. It slows the sender down by periodically blocking the sender.<p>Where in UDP applications where peers, TCP follows a strict client-server model. The server selects a port, sets up a <strong>server socket</strong> with that port, then starts <em>listening</em> on that socket. A client then sets up a socket and then tries to connect with the server by sending a message to the server on that port. The server sends an acceptance of that request and then the client responds with an acknowledgement. At that point, the server sets up a new socket that is <em>just</em> connected to that client. Now the client's socket and the server's new socket act like a pipe.<p>The server gets a new socket to allow the server socket to keep listening for new connections.<h5 id=tcp-in-java><a href=#tcp-in-java aria-label="anchor link for tcp-in-java">#####</a> TCP in Java</h5><p>This requires <code>java.net.ServerSocket</code> and <code>java.net.Socket</code>. The client creates a <code>Socket</code> with the given host and port. The server creates a <code>ServerSocket</code> which is basically a factory for <code>Socket</code>s; the server repeatedly calls <code>ServerSocket.accept</code> to generate <code>Socket</code>s that correspond to individual client sockets.<h3 id=application-layer><a href=#application-layer aria-label="anchor link for application-layer">###</a> Application Layer</h3><p>This is how your application actually communicates and is what you think about most. It includes things like HTTP, SSH, and DNS.<p>As the name implies, this is not the job of the OS.<h2 id=remote-procedure-call-rpc><a href=#remote-procedure-call-rpc aria-label="anchor link for remote-procedure-call-rpc">##</a> Remote Procedure Call (RPC)</h2><p>Often, we don't structure our code as sends and receives. Instead, we think about code in terms of functions. RPC makes this possible. Essentially, you have some framework / library that can transparently send and receive network messages and make them look as though they were executed on the same host like a normal function (hence transparent).<p>This is normally done by having a server with a thin client-stub that exposes all functions the server can perform. This client-stub could even be auto-generated. Then, whenever you want to do an RPC call, you call the client-stub that makes a request to the server.<p>RPC could easily be implemented using UDP using one-off requests which should be faster (assuming everything works). However, this is dangerous because you don't know whether the request or the response got lost. In other words, you don't know whether your function was executed or not. To cope with that, we want <strong>idempotent</strong> functions or functions that don't have any additional affect after being called more than once. Alternatively, we could guarantee that call functions either zero or one times. That is, we don't retransmit requests.<h2 id=structures-for-distributed-applications><a href=#structures-for-distributed-applications aria-label="anchor link for structures-for-distributed-applications">##</a> Structures for Distributed Applications</h2><h3 id=client-server-model><a href=#client-server-model aria-label="anchor link for client-server-model">###</a> Client-Server Model</h3><p>One of the oldest and most widely used model of distributed applications is client-server. You have one server and many different clients. It is <em>highly centralized</em>.<p>We normally split up applications into layers. A common split is presentation logic, application logic, database logic, and database management system (DBMS). We can then decide where we put each layer. Where we want to put the layers depends on the structure of our application, but here's a list of some common named splits.<ul><li>Host-based Processing: All layers on the server.<li>Server-based Processing: Presentation logic on client. All others on server.<li>Client-based Processing: Presentation, application, and (some) database logic on client. DBMS done by server.<li>Cooperative Processing: Presentation logic and some application logic on client. Some application logic on server and database logic and DBMS on server.<ul><li>Shared computational load!<li>More complex to design.</ul></ul><h3 id=peer-to-peer><a href=#peer-to-peer aria-label="anchor link for peer-to-peer">###</a> Peer-to-peer</h3><p>There is no server. Everyone is a "client" and shares in part of the application logic.<p>Scales very well but often more complicated to design.<h3 id=transactional><a href=#transactional aria-label="anchor link for transactional">###</a> Transactional</h3><p>Model all application logic in terms of transactions. A users then just makes various transactions with the system and the system guarantees they are executed appropriately.<h1 id=protection><a href=#protection aria-label="anchor link for protection">#</a> Protection</h1><p>For many computing scenarios we want to enforce a certain <strong>policy</strong> on how OS resources can be used and by who. For example, maybe we don't want everyone to be able to read certain files or maybe we want to restrict how much a certain group can print.<p>Normally, when implementing and designing protection solutions, we talk about <strong>access rights</strong> (rules) to the <strong>operations</strong> certain users can perform on given <strong>objects</strong> (resources). <em>Note:</em> Instead of saying users, we normally call the things with the access rights a <strong>protection domain</strong> because sometimes we deal with groups or programs or other things.<p>A good permissions system allows for many different types of permission policies and fine grain control.<h2 id=access-rights><a href=#access-rights aria-label="anchor link for access-rights">##</a> Access Rights</h2><p>Some common access rights are read, write, execute, append, create, delete, list (for directories). There can be access rights specific to various objects, such as printing for a printer. However, this is often handled using the standard file system access rights. (For example, printing to a printer might be write permissions.)<p>Often we conditionally allow domains to give away access rights to other domains. We may add an additional access right for all normal ones, for example read* (read star). Here the star means that the domain can give that permission to other domains (limited copy) or transfer that star access right to another domain. However, another more often used technique is to consider certain domains the <strong>owner</strong> of the object. The owner can give away and revoke access rights as it sees fit.<p>Another trick that is sometimes used is the <strong>switch</strong> access right to a domain object. This is used to allow certain domains to temporarily switch to another domain and use its access rights. However, this leads to the <strong>confinement problem</strong>, where domains can switch to another domain which has access to things the original domain shouldn't have access to. Even worse, sometimes this switch right chain can be hard to find, it is even undecidable if you allow creation of domains and objects.<p><em>How do we record access rights?</em><h3 id=access-matrix><a href=#access-matrix aria-label="anchor link for access-matrix">###</a> Access Matrix</h3><p>One naive / easy way would be to use an <strong>access matrix</strong>, where every row in a protection domain, every column is an object, and the internal cells are the access rights of that domain.<p>This is extremely memory inefficient because matrices take a lot of memory and are very sparse. As such, it is very rarely used. However, it is easy to see.<h3 id=access-list><a href=#access-list aria-label="anchor link for access-list">###</a> Access List</h3><ul><li>Easy to know who can use an object.<li>Easy to remove an object or revoke rights.<li>Hard to answer "What can I access?"<li>Most common in general.</ul><p>Basically, you convert each column in the access matrix into a list. Every list is associated with an object and contains a series of entries, each containing a domain and its access rights. You omit empty spaces which allows for much less wasted space.<p>Each object keeps track of its individual access list (aka column).<h3 id=capability-list><a href=#capability-list aria-label="anchor link for capability-list">###</a> Capability List</h3><ul><li>Easy to know what a domain can access.<li>Hard to remove an object or revoke rights.<li>Hard to answer "Who can access this?"<li>Who manages the list??</ul><p>A capability list is a lot like an access list except that it lists the rows. Every list is associated with a domain and contains a series of entries, each containing its object and its access right. You omit empty spaces which allows for much less wasted space.<p>Each domain keeps track of its individual capability list (aka row).<h3 id=lock-and-key><a href=#lock-and-key aria-label="anchor link for lock-and-key">###</a> Lock and Key</h3><ul><li>Hard to know who can access an object.<li>Hard to know what objects a domain can access.<li>Easy to revoke access rights.<li>Maybe more efficient to represent and give out access rights.</ul><p>Lock and key is like a mixture of access list and capability list. Every domain contains a list of keys and every object contains a list of locks. Then, to find out what your access rights are, a domain matches its key with an objects lock. We're normally clever, having domains share keys with the same permissions and having objects use the same keys as other objects.<p><em>Note:</em> it is permissible for a domain to have multiple keys to the same object with the same access rights. Basically, it's okay if you have multiple reasons for accessing an object.<p>This is good in general because it enables easier revoking of access rights, by having keys be <em>reasons</em> that a domain can access that resource. Then, you just revoke that key so you don't accidentally take aways a domain's permissions to an object when it had multiple reasons of using it.<p><em>Note:</em> There are many different ways to convert an access matrix into a lock and key system.<p><em>Note:</em> This is kinda how Unix group ownership works.<h2 id=standard-unix-permissions><a href=#standard-unix-permissions aria-label="anchor link for standard-unix-permissions">##</a> Standard Unix Permissions</h2><p>Every user has a unique user id. Every user is a member in multiple groups. Every group has a unique group id.<p>Permissions are associated with each object. Every object has an owner, group, and mode bits. There are three mode bits assigned to each of the three domains. They are read, write, and execute as the permissions mode bits and owner, group, and other as the domains.<p>For files, read, write, and execute mean exactly what you'd think. For directories, read means you can get a listing, write means you can add / delete files, and execute means you can see file attributes.<p>Your groups are kinda like a list of keys and the single group for an object is kinda like a single lock for that object. You have to squint a bit though.<p>There's also some extra bits beyond the mode bits we normally think about. There's the setuid bit which means pretend to be the owner while executing, the setgid bit which means pretend to be the group while executing, and the sticky bit which means keep this program in memory even when it's not running. The sticky bit used to be useful to keep commonly used programs in memory for efficiency, but it's not as important anymore because OS caching has improved and we have faster computers.<h3 id=syscalls-for-unix-permissions><a href=#syscalls-for-unix-permissions aria-label="anchor link for syscalls-for-unix-permissions">###</a> Syscalls for Unix Permissions</h3><ul><li><code>fchmod(int fd, mode_t mode)</code>: Set mode bits of file.<li><code>fchown(int fd, uid_t owner, gid_t group)</code>: Set owner and group of file.<li><code>fstat(int fd, struct stat *buf)</code>: Fill in <code>buf</code> with information about the file.</ul><h2 id=permissions-and-programs><a href=#permissions-and-programs aria-label="anchor link for permissions-and-programs">##</a> Permissions and Programs</h2><p>Most programs are run with exactly the same permissions as you when running. In other words, the program is you.<p>Some security techniques such as sandboxing change this by running the program in a limited environment to help prevent trojan horses and other exploits. The principle behind sandboxing is called the <strong>principle of least privilege</strong>, which says we should only give programs exactly the permissions they need and no more. This helps prevent accidental breakages either from mistakes or malicious programs. You can do this in a Unix-y way using groups, but that is limited in terms of how many groups you can have.<p>To do this truly well, we'd like programs to have some way of dynamically changing their protection domain. That way, programs could start with as few permissions as possible and assign certain module / subprocesses of the program high permissions to allow them to do just what they want to do. For example, this would allow the init system to have very few permissions but spawn off higher permission processes.<h3 id=permissions-and-java><a href=#permissions-and-java aria-label="anchor link for permissions-and-java">###</a> Permissions and Java</h3><p>Java actually has an interesting concept of "trusted" code. By default, code from your hard drive is considered code, but it is possible that the JVM downloads code and starts executing it while it is running. In this case, it considers the downloaded code untrusted and doesn't allow it to do certain protected operations (e.g. opening arbitrary files).<p>This is enforced using stack inspection from the <code>AccessController</code> class. It has a <code>checkPermissions</code> method which introspects the stack to find out if we are allowed to do privileged code. It has the <code>doPrivileged</code> method, which only trusted code can execute and marks the current point in the stack as a point where a privileged operation was allowed.</article>